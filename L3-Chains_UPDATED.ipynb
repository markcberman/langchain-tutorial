{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa12c33a",
   "metadata": {},
   "source": [
    "# L3 — Chains and Runnables (LangChain v1) — **Single End‑to‑End Chain**\n",
    "\n",
    "This rewritten notebook builds **one composed LCEL chain** that runs the whole workflow:\n",
    "\n",
    "**topic → outline → expanded paragraph → summary**\n",
    "\n",
    "It also shows *exactly* how values are passed between steps using LCEL and `RunnablePassthrough.assign(...)`.\n",
    "\n",
    "> You can call the final chain with either a **string** (`\"my topic\"`) or a **dict** (`{\"topic\": \"my topic\"}`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a3141",
   "metadata": {},
   "source": [
    "## 1) Setup\n",
    "\n",
    "Make sure you have an OpenAI API key in your environment:\n",
    "\n",
    "- `OPENAI_API_KEY`\n",
    "\n",
    "Then restart the kernel so the environment variable is picked up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6512f2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-5-mini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Set OPENAI_API_KEY in your environment and restart the kernel.\"\n",
    "\n",
    "MODEL = \"gpt-5-mini\"  # change if you want\n",
    "print(\"Using model:\", MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad051b6",
   "metadata": {},
   "source": [
    "## 2) Imports\n",
    "\n",
    "Key concepts used below:\n",
    "\n",
    "- **LCEL**: the `|` operator builds a runnable pipeline.\n",
    "- **RunnableLambda**: wraps a Python function as a runnable.\n",
    "- **RunnablePassthrough.assign(...)**: adds new keys to a dict while keeping existing keys.\n",
    "- **StrOutputParser**: converts the LLM response into a Python `str`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115515a0",
   "metadata": {},
   "source": [
    "## 3) Create the model and output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a92dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=MODEL)\n",
    "to_text = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fabc512",
   "metadata": {},
   "source": [
    "## 4) Prompts\n",
    "\n",
    "Each prompt below expects a specific **input key**:\n",
    "\n",
    "- Outline prompt expects: `{\"topic\": ...}`\n",
    "- Expand prompt expects: `{\"outline\": ...}`\n",
    "- Summary prompt expects: `{\"text\": ...}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b50c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Create a compact outline with 4 bullet points.\"),\n",
    "    (\"user\", \"{topic}\")\n",
    "])\n",
    "\n",
    "expand_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Expand bullet point #2 into a short paragraph.\"),\n",
    "    (\"user\", \"Outline:\\n{outline}\")\n",
    "])\n",
    "\n",
    "summ_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Summarize the text in 2 sentences.\"),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae090cca",
   "metadata": {},
   "source": [
    "## 5) Build small chains\n",
    "\n",
    "These are the *atomic* pieces:\n",
    "\n",
    "- `outline_chain`: dict with `topic` → outline string  \n",
    "- `expand_chain`: dict with `outline` → expanded paragraph string  \n",
    "- `summ_chain`: dict with `text` → summary string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6c1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_chain = outline_prompt | llm | to_text\n",
    "expand_chain = expand_prompt | llm | to_text\n",
    "summ_chain = summ_prompt | llm | to_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbade86",
   "metadata": {},
   "source": [
    "## 6) Compose one end‑to‑end chain (LCEL)\n",
    "\n",
    "### The pattern\n",
    "\n",
    "We keep the input as a **dict** and progressively *add* new keys:\n",
    "\n",
    "1. Ensure input is a dict with key `topic`.\n",
    "2. Add `outline` by running `outline_chain` on the dict.\n",
    "3. Add `expanded` by running `expand_chain` on a dict containing only the `outline`.\n",
    "4. Add `summary` by running `summ_chain` on a dict containing only the `text`.\n",
    "\n",
    "### Why it’s less confusing\n",
    "\n",
    "- Every step has a clear input/output shape.\n",
    "- `assign(...)` preserves the running “state” dict so you can see everything at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ecfbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda x: {'topic': x} if isinstance(x, str) else x)\n",
       "| RunnableAssign(mapper={\n",
       "    outline: ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Create a compact outline with 4 bullet points.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='{topic}'), additional_kwargs={})])\n",
       "             | ChatOpenAI(profile={'max_input_tokens': 272000, 'max_output_tokens': 128000, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x11203f110>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11203f4d0>, root_client=<openai.OpenAI object at 0x11203e710>, root_async_client=<openai.AsyncOpenAI object at 0x11203f250>, model_name='gpt-5-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "             | StrOutputParser()\n",
       "  })\n",
       "| RunnableAssign(mapper={\n",
       "    expanded: RunnableLambda(lambda d: {'outline': d['outline']})\n",
       "              | ChatPromptTemplate(input_variables=['outline'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Expand bullet point #2 into a short paragraph.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['outline'], input_types={}, partial_variables={}, template='Outline:\\n{outline}'), additional_kwargs={})])\n",
       "              | ChatOpenAI(profile={'max_input_tokens': 272000, 'max_output_tokens': 128000, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x11203f110>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11203f4d0>, root_client=<openai.OpenAI object at 0x11203e710>, root_async_client=<openai.AsyncOpenAI object at 0x11203f250>, model_name='gpt-5-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "              | StrOutputParser()\n",
       "  })\n",
       "| RunnableAssign(mapper={\n",
       "    summary: RunnableLambda(lambda d: {'text': d['expanded']})\n",
       "             | ChatPromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Summarize the text in 2 sentences.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})])\n",
       "             | ChatOpenAI(profile={'max_input_tokens': 272000, 'max_output_tokens': 128000, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x11203f110>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11203f4d0>, root_client=<openai.OpenAI object at 0x11203e710>, root_async_client=<openai.AsyncOpenAI object at 0x11203f250>, model_name='gpt-5-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "             | StrOutputParser()\n",
       "  })"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accept either:\n",
    "# - a raw string topic: \"my topic\"\n",
    "# - or a dict: {\"topic\": \"my topic\"}\n",
    "normalize_input = RunnableLambda(\n",
    "    lambda x: {\"topic\": x} if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Helper runnables to reshape the dict for downstream chains\n",
    "to_outline_input = RunnableLambda(lambda d: {\"outline\": d[\"outline\"]})\n",
    "to_summary_input = RunnableLambda(lambda d: {\"text\": d[\"expanded\"]})\n",
    "\n",
    "end_to_end_chain = (\n",
    "    normalize_input\n",
    "    # Add outline while keeping {\"topic\": ...}\n",
    "    | RunnablePassthrough.assign(outline=outline_chain)\n",
    "    # Add expanded paragraph (expand_chain expects {\"outline\": ...})\n",
    "    | RunnablePassthrough.assign(expanded=(to_outline_input | expand_chain))\n",
    "    # Add summary (summ_chain expects {\"text\": ...})\n",
    "    | RunnablePassthrough.assign(summary=(to_summary_input | summ_chain))\n",
    ")\n",
    "\n",
    "end_to_end_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2713fb",
   "metadata": {},
   "source": [
    "## 7) Run it\n",
    "\n",
    "The output is a single dict containing all intermediate artifacts:\n",
    "\n",
    "- `topic`\n",
    "- `outline`\n",
    "- `expanded`\n",
    "- `summary`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3254a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOPIC ===\n",
      "How to write reliable prompts for LLMs\n",
      "\n",
      "=== OUTLINE ===\n",
      "- State the goal and success criteria up front: name the task, the model’s role (e.g., \"act as a technical reviewer\"), the required audience, and the exact output format you want (JSON, bullet list, code, length).\n",
      "- Give essential context and examples: include all facts the model needs and 1–3 few‑shot examples or templates so it learns the desired structure and content.\n",
      "- Add explicit constraints and style rules: specify tone, level of detail, forbidden content, stepwise reasoning if needed (e.g., \"show steps\" vs \"only final answer\"), and any formatting rules.\n",
      "- Iterate and validate: test with varied inputs, run adversarial/edge cases, refine prompts that produce errors, and use temperature/beam settings or follow‑up verification prompts to improve reliability.\n",
      "\n",
      "=== EXPANDED (bullet #2) ===\n",
      "Give the model all essential context and concrete examples so it knows what to produce: include the relevant facts, background, definitions, data formats, and any variable values or constraints it must assume. Provide 1–3 few‑shot examples or templates—real inputs paired with ideal outputs—that demonstrate the exact structure, tone, and level of detail you want, and show how to handle common edge cases or ambiguous inputs. When helpful, mark optional fields or placeholders and vary the examples to cover typical variations and failure modes so the model can generalize the pattern.\n",
      "\n",
      "=== SUMMARY ===\n",
      "Provide the model with complete, concrete context (relevant facts, background, definitions, data formats, variable values/constraints) and include 1–3 few‑shot examples or templates—real inputs paired with ideal outputs—that demonstrate the exact structure, tone, and level of detail you expect, plus guidance for handling common edge cases or ambiguous inputs. Mark optional fields/placeholders and vary the examples to cover typical variations and failure modes so the model can generalize the pattern.\n"
     ]
    }
   ],
   "source": [
    "topic = \"How to write reliable prompts for LLMs\"\n",
    "\n",
    "result = end_to_end_chain.invoke(topic)  # you can also pass {\"topic\": topic}\n",
    "\n",
    "print(\"=== TOPIC ===\")\n",
    "print(result[\"topic\"])\n",
    "print(\"\\n=== OUTLINE ===\")\n",
    "print(result[\"outline\"])\n",
    "print(\"\\n=== EXPANDED (bullet #2) ===\")\n",
    "print(result[\"expanded\"])\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(result[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fd0c1",
   "metadata": {},
   "source": [
    "## 8) (Optional) Inspect what flows through the chain\n",
    "\n",
    "If you’re learning LCEL, it can help to print the “state dict” after each stage.\n",
    "Below we build the same chain in 3 visible steps so you can see the shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f29c076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state0: {'topic': 'How to write reliable prompts for LLMs'}\n",
      "\n",
      "state1 keys: dict_keys(['topic', 'outline'])\n",
      "\n",
      "state2 keys: dict_keys(['topic', 'outline', 'expanded'])\n",
      "\n",
      "state3 keys: dict_keys(['topic', 'outline', 'expanded', 'summary'])\n"
     ]
    }
   ],
   "source": [
    "state0 = normalize_input.invoke(topic)\n",
    "print(\"state0:\", state0)\n",
    "\n",
    "state1 = (normalize_input | RunnablePassthrough.assign(outline=outline_chain)).invoke(topic)\n",
    "print(\"\\nstate1 keys:\", state1.keys())\n",
    "\n",
    "state2 = (\n",
    "    normalize_input\n",
    "    | RunnablePassthrough.assign(outline=outline_chain)\n",
    "    | RunnablePassthrough.assign(expanded=(to_outline_input | expand_chain))\n",
    ").invoke(topic)\n",
    "print(\"\\nstate2 keys:\", state2.keys())\n",
    "\n",
    "state3 = end_to_end_chain.invoke(topic)\n",
    "print(\"\\nstate3 keys:\", state3.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee273a9d-e76f-490a-b2ef-65a1ab6c8fc9",
   "metadata": {},
   "source": [
    "### Same Logic Without LCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32587abc-797d-4f5b-9af5-ef2653ed89db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOPIC ===\n",
      "How to write reliable prompts for LLMs\n",
      "\n",
      "=== OUTLINE ===\n",
      "- State the goal, role, audience, and explicit success criteria (what the output must accomplish).\n",
      "- Give necessary context and concrete examples or input-output pairs so the model knows desired content and tone.\n",
      "- Specify format, length, style, constraints, and any stepwise process (e.g., \"list, 5 items, concise, include sources\").\n",
      "- Test with edge cases, measure output quality, iterate prompts, and add guardrails (clarifications, rejection rules, sampling/temperature settings).\n",
      "\n",
      "=== EXPANDED (bullet #2) ===\n",
      "Provide the model with all necessary background and plenty of concrete examples so it clearly understands the desired content and tone: explain relevant domain facts, the audience’s prior knowledge, any constraints or assumptions, and then show sample inputs with ideal outputs (and, if helpful, counter-examples of what to avoid). Include examples that illustrate structure, phrasing, level of detail, and voice (formal vs. conversational), and cover typical and edge-case scenarios so the model can generalize the pattern rather than guess.\n",
      "\n",
      "=== SUMMARY ===\n",
      "Provide the model with comprehensive background and concrete examples — including relevant domain facts, the audience’s prior knowledge, and any constraints or assumptions — so it clearly understands the desired content and tone. Also include sample inputs with ideal outputs and counter-examples that illustrate structure, phrasing, level of detail, voice (formal vs. conversational), and both typical and edge-case scenarios to help the model generalize rather than guess.\n"
     ]
    }
   ],
   "source": [
    "topic = \"How to write reliable prompts for LLMs\"\n",
    "\n",
    "def end_to_end_plain(raw_input):\n",
    "    # normalize_input\n",
    "    if isinstance(raw_input, str):\n",
    "        state = {\"topic\": raw_input}\n",
    "    else:\n",
    "        state = raw_input\n",
    "\n",
    "    # outline\n",
    "    state[\"outline\"] = outline_chain.invoke(state)\n",
    "\n",
    "    # expanded\n",
    "    expand_input = {\"outline\": state[\"outline\"]}\n",
    "    state[\"expanded\"] = expand_chain.invoke(expand_input)\n",
    "\n",
    "    # summary\n",
    "    summary_input = {\"text\": state[\"expanded\"]}\n",
    "    state[\"summary\"] = summ_chain.invoke(summary_input)\n",
    "\n",
    "    return state\n",
    "\n",
    "result = end_to_end_plain(topic)\n",
    "\n",
    "print(\"=== TOPIC ===\")\n",
    "print(result[\"topic\"])\n",
    "print(\"\\n=== OUTLINE ===\")\n",
    "print(result[\"outline\"])\n",
    "print(\"\\n=== EXPANDED (bullet #2) ===\")\n",
    "print(result[\"expanded\"])\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(result[\"summary\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174941e-db1f-485f-b4aa-3484eb060657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

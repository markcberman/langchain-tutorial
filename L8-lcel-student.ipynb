{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc925562",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044db1b7-c18c-45d2-be7a-29f027c901e2",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables (expects OPENAI_API_KEY to be set in your environment or .env)\n",
    "_ = load_dotenv(find_dotenv())\n",
    "assert os.environ.get(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY is not set\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd55c0a0-ca4e-4311-a33c-fcebeb7d8b1e",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c432b-b2c9-497b-9912-c9ca4c1e3740",
   "metadata": {},
   "source": [
    "## Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0be20d-0e00-478c-a844-017cad13af22",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aedf1c1e-b697-47ce-9d81-eaec9192243b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df32028-d35f-4392-bb15-ddeec9ee09b5",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What do you call a bear with no teeth? A gummy bear!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba33b5-2ae9-44d7-b023-18c903af571a",
   "metadata": {},
   "source": [
    "## More complex chain\n",
    "\n",
    "And Runnable Map to supply user-provided inputs to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d036bb8e-8ca7-4dbd-8103-f50a3c8c3af9",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8955cff7-f1a2-4f94-ab5b-fcdda0859702",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "    collection_name=\"lcel_demo\",\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2df87934-1697-405c-b460-5e9bfd16c792",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='harrison worked at kensho'),\n",
       " Document(metadata={}, page_content='bears like to eat honey')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "871cb26b-97b3-4f63-8bb3-523d3e6d117b",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='bears like to eat honey'),\n",
       " Document(metadata={}, page_content='harrison worked at kensho')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what do bears like to eat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "127a7fb6-5821-4934-ab56-9e3300516c05",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec01c56-731c-4e4f-a5f6-493fba953db0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9ca6506-826f-4420-8f19-25dd4dbbc1dc",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "707d1319-8840-4ed5-b4a4-a2a128799db6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05ec3727-4284-417e-9e23-eec0682eb002",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "inputs = RunnableMap({\n",
    "    \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4216c7ab-6d1b-4f2a-98dc-5d2ace23e3c2",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={}, page_content='harrison worked at kensho'),\n",
       "  Document(metadata={}, page_content='bears like to eat honey')],\n",
       " 'question': 'where did harrison work?'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec59c3b-33e7-437f-9b8b-b4652bc3b863",
   "metadata": {},
   "source": [
    "## Bind\n",
    "\n",
    "and OpenAI tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d335049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "class WeatherSearchInput(BaseModel):\n",
    "    airport_code: str = Field(..., description=\"The airport code to get the weather for\")\n",
    "\n",
    "@tool(args_schema=WeatherSearchInput)\n",
    "def weather_search(airport_code: str) -> str:\n",
    "    \"\"\"Search for weather given an airport code.\"\"\"\n",
    "    # Dummy implementation for teaching/demo purposes\n",
    "    return f\"The weather at {airport_code.upper()} is sunny.\"\n",
    "\n",
    "class SportsSearchInput(BaseModel):\n",
    "    team_name: str = Field(..., description=\"The sports team to search for\")\n",
    "\n",
    "@tool(args_schema=SportsSearchInput)\n",
    "def sports_search(team_name: str) -> str:\n",
    "    \"\"\"Search for news of recent sport events.\"\"\"\n",
    "    # Dummy implementation for teaching/demo purposes\n",
    "    return f\"Latest result: {team_name} won their last game.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a8b8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"{input}\")])\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-5-mini\", temperature=0).bind_tools([weather_search, sports_search])\n",
    "runnable = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "887d53a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 182, 'total_tokens': 207, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CsccexifcrIbCTRimPLOWDTgwKIPY', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b7168-3095-7d10-9c8c-b8e39b9a01af-0', tool_calls=[{'name': 'weather_search', 'args': {'airport_code': 'SFO'}, 'id': 'call_oFfm0c4sz2VC3nQ6qpNxKvH8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 182, 'output_tokens': 25, 'total_tokens': 207, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"What is the weather at SFO? Use the weather_search tool.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be336800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 185, 'total_tokens': 276, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Csccg4hjDnzHOdwJJIoGENuGh4iXd', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b7168-363a-7ee2-bf99-8da8c9c4449d-0', tool_calls=[{'name': 'sports_search', 'args': {'team_name': 'New England Patriots'}, 'id': 'call_r01Cw1eU4ZTyPHnV34gNxlzP', 'type': 'tool_call'}], usage_metadata={'input_tokens': 185, 'output_tokens': 91, 'total_tokens': 276, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"How did the Patriots do in their most recent game? Use the sports_search tool.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bef4c0",
   "metadata": {},
   "source": [
    "## Fallbacks and JSON reliability patterns\n",
    "\n",
    "This section shows three ways to make “LLM → JSON” workflows reliable:\n",
    "\n",
    "1) **Fallbacks**: if parsing fails, retry with a stricter chain.\n",
    "2) **Structured outputs (schema-guided)**: guide the model to output JSON matching a schema (no fallback needed).\n",
    "3) **JSON repair**: insert a repair stage between `StrOutputParser()` and `json.loads`.\n",
    "\n",
    "All examples below are **self-contained**: they define any variables they need inside this section,\n",
    "so they don’t depend on cells later in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcf807b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Use the notebook's `model` if it already exists, otherwise create a local one.\n",
    "try:\n",
    "    fallback_model = model  # noqa: F821\n",
    "except NameError:\n",
    "    fallback_model = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e00f62",
   "metadata": {},
   "source": [
    "### A) Fallbacks (a real recovery example)\n",
    "\n",
    "**Goal:** parse JSON reliably even when the model sometimes includes extra text.\n",
    "\n",
    "- **Primary chain** is *designed to fail* JSON parsing by asking for an explanation **before** the JSON.\n",
    "  That makes the output invalid JSON for `json.loads`.\n",
    "- **Fallback chain** repeats the task with a stricter instruction: **return ONLY JSON**.\n",
    "- `with_fallbacks([fallback_chain])` runs the fallback automatically when the primary chain throws an exception.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e63a9cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poems': ['Paws press on soft moss\\nAmber breath fogs the cold air\\nRiver remembers',\n",
       "  'Cub tumbles in sun\\nDandelion crowns, sticky paws\\nNap in a honeyed dream',\n",
       "  'Claw marks on the cliff\\nYears counted in salmon bones\\nNight sky tucks him home']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primary prompt intentionally produces non-JSON text + JSON => json.loads will fail.\n",
    "bad_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"\"\"Write 3 short poems about bears.\n",
    "First, explain briefly what you will do.\n",
    "Then provide a JSON object with key: poems (array of 3 strings).\"\"\")\n",
    "])\n",
    "\n",
    "primary_chain = bad_prompt | fallback_model | StrOutputParser() | json.loads\n",
    "\n",
    "# Fallback prompt forces ONLY JSON.\n",
    "good_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Return ONLY valid JSON. No extra text. No markdown fences.\"),\n",
    "    (\"user\", \"\"\"Write 3 short poems about bears.\n",
    "Return a JSON object with key:\n",
    "- poems: array of 3 strings\"\"\")\n",
    "])\n",
    "\n",
    "fallback_chain = good_prompt | fallback_model | StrOutputParser() | json.loads\n",
    "\n",
    "final_chain = primary_chain.with_fallbacks([fallback_chain])\n",
    "\n",
    "result_fallback = final_chain.invoke({})\n",
    "result_fallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f39b1",
   "metadata": {},
   "source": [
    "Optional: show the exception that triggers the fallback (for learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7642080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/l9/3b84hrtx38sd0981n54901g80000gn/T/ipykernel_1808/1405841695.py\", line 2, in <module>\n",
      "    primary_chain.invoke({})\n",
      "    ~~~~~~~~~~~~~~~~~~~~^^^^\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 3151, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 4880, in invoke\n",
      "    return self._call_with_config(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._invoke,\n",
      "        ^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 2058, in _call_with_config\n",
      "    context.run(\n",
      "    ~~~~~~~~~~~^\n",
      "        call_func_with_variable_args,  # type: ignore[arg-type]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    ),\n",
      "    ^\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/runnables/config.py\", line 433, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 4737, in _invoke\n",
      "    output = call_func_with_variable_args(\n",
      "        self.func, input_, config, run_manager, **kwargs\n",
      "    )\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/runnables/config.py\", line 433, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/json/__init__.py\", line 352, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/json/decoder.py\", line 345, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/json/decoder.py\", line 363, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    primary_chain.invoke({})\n",
    "except Exception:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d3c6c",
   "metadata": {},
   "source": [
    "### B) Without fallbacks: structured outputs (schema-guided)\n",
    "\n",
    "**How it works**\n",
    "- Define the output shape using a **Pydantic v2** model.\n",
    "- `JsonOutputParser` generates *format instructions* that you inject into the prompt.\n",
    "- The model is guided to output JSON matching the schema.\n",
    "- The parser validates and returns structured data.\n",
    "\n",
    "This typically avoids JSON parsing failures because the model is strongly constrained to produce valid JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31ed689c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poems': ['Forest hush, brown giant\\npaws press the earth, slow and sure\\nhoney-sweet dawn.',\n",
       "  'Winter curled, white breath\\nsleep thick as the fallen snow\\ndreams of rivers.',\n",
       "  \"Cub tumbles in sun\\nmother's shadow watches close\\nlaughter in the grass.\"]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Poems(BaseModel):\n",
    "    poems: List[str] = Field(..., description=\"Three short poems about bears\")\n",
    "\n",
    "structured_parser = JsonOutputParser(pydantic_object=Poems)\n",
    "\n",
    "structured_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Follow the format instructions exactly.\"),\n",
    "    (\"user\", \"\"\"Write 3 short poems about bears.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "]).partial(format_instructions=structured_parser.get_format_instructions())\n",
    "\n",
    "structured_chain = structured_prompt | fallback_model | structured_parser\n",
    "\n",
    "result_structured = structured_chain.invoke({})\n",
    "result_structured\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012b457",
   "metadata": {},
   "source": [
    "### C) JSON repair step between `StrOutputParser()` and `json.loads`\n",
    "\n",
    "**How it works**\n",
    "- Keep a generation chain that might emit messy text.\n",
    "- Attempt `json.loads(text)`.\n",
    "- If it fails, call a *repair chain* that rewrites the text into **valid JSON only**.\n",
    "- Parse the repaired JSON.\n",
    "\n",
    "This adds robustness when you can’t fully control the upstream prompt.\n",
    "Tradeoff: it may require a second model call when repair is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "defcae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poems': ['Honey-scented dusk\\nheavy paws print the soft earth-\\nmoon tucks in the den',\n",
       "  'A cub flops, surprised,\\nclumsy joy unspools in green,\\nwind teaches it to roar',\n",
       "  'River-silvered eyes,\\nan old bear remembers seasons—\\nstones keep his stories']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repair_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a JSON repair assistant. Return ONLY valid JSON. No extra text. No markdown fences.\"),\n",
    "    (\"user\", \"\"\"Fix the following text into valid JSON that matches this schema:\n",
    "\n",
    "Schema:\n",
    "- poems: array of 3 strings\n",
    "\n",
    "Text to repair:\n",
    "{text}\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "repair_chain = repair_prompt | fallback_model | StrOutputParser()\n",
    "\n",
    "def parse_or_repair(text: str):\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        repaired = repair_chain.invoke({\"text\": text})\n",
    "        return json.loads(repaired)\n",
    "\n",
    "json_repair_step = RunnableLambda(parse_or_repair)\n",
    "\n",
    "# A \"bad\" generator likely to include extra non-JSON text.\n",
    "bad_generation = bad_prompt | fallback_model | StrOutputParser()\n",
    "\n",
    "repaired_chain = bad_generation | json_repair_step\n",
    "\n",
    "result_repaired = repaired_chain.invoke({})\n",
    "result_repaired\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d178d295",
   "metadata": {},
   "source": [
    "### Combine with `with_fallbacks` and invoke\n",
    "\n",
    "If `primary_chain` errors, LangChain automatically runs `fallback_chain` with the same input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "711e93eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poems': ['Mountain shadow moves\\nHeavy breath in berry dusk\\nWinter hush listens',\n",
       "  'She pads through thawing streams,\\nBroad shoulders brushing river light.\\nA thumb of honey on her tongue,\\nMemory of sleep in the soil.',\n",
       "  'Cub rolls like a barrel\\nChasing leaves and summer sun;\\nMother laughs in the pines.']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = primary_chain.with_fallbacks([fallback_chain])\n",
    "\n",
    "result = final_chain.invoke({})\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b1d62",
   "metadata": {},
   "source": [
    "### Optional: show the error that triggers the fallback\n",
    "\n",
    "This cell shows the exception thrown by the primary chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35bb340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/l9/3b84hrtx38sd0981n54901g80000gn/T/ipykernel_1808/1405841695.py\", line 2, in <module>\n",
      "    primary_chain.invoke({})\n",
      "    ~~~~~~~~~~~~~~~~~~~~^^^^\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 3151, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 4880, in invoke\n",
      "    return self._call_with_config(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._invoke,\n",
      "        ^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 2058, in _call_with_config\n",
      "    context.run(\n",
      "    ~~~~~~~~~~~^\n",
      "        call_func_with_variable_args,  # type: ignore[arg-type]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    ),\n",
      "    ^\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/runnables/config.py\", line 433, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 4737, in _invoke\n",
      "    output = call_func_with_variable_args(\n",
      "        self.func, input_, config, run_manager, **kwargs\n",
      "    )\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/runnables/config.py\", line 433, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/json/__init__.py\", line 352, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/json/decoder.py\", line 345, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/markberman/miniconda3/envs/langchain/lib/python3.13/json/decoder.py\", line 363, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    primary_chain.invoke({})\n",
    "except Exception:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfdda0-3db2-4073-a647-f2d62c460349",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33b3b27f-b5a0-4db5-a1b0-20754437a47e",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48c8cabf-ea55-4448-b070-3ec22942c559",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What do you call a bear with no teeth? A gummy bear.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc58bdb4-a896-46ba-90c4-1b333245229a",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What do you call a bear with no teeth? A gummy bear.',\n",
       " \"Why are frogs so happy? Because they eat whatever's bugging them.\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a069d61-0a67-4368-b7c4-367262267bb8",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What\n",
      " do\n",
      " you\n",
      " call\n",
      " a\n",
      " bear\n",
      " with\n",
      " no\n",
      " teeth\n",
      "?\n",
      " A\n",
      " gummy\n",
      " bear\n",
      ".\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2315b43f-c7e1-4f7b-9595-4cabdc019dea",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What do you call a bear with no teeth? A gummy bear.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3082379-f75e-4a74-bee5-c18ddc5ac4dc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adccca48-787e-4219-af82-f18e6408182b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c816827-e5b7-480a-826b-0ca715faca3c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e6d608-c205-4141-bab8-0304dd910978",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc490d-090d-4e1c-b8ee-bf40ec4da4c3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe48ff0-fd84-4c9e-8069-02117d57f7f0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a4bfd3",
   "metadata": {},
   "source": [
    "# Conversational agent (LangChain 1.2.x / 2025–2026 style)\n",
    "\n",
    "This notebook is a **drop-in modernization** of a 2023 LangChain tutorial that used now-obsolete patterns like:\n",
    "- legacy `langchain.chat_models.ChatOpenAI`\n",
    "- OpenAI *function-calling* utilities like `format_tool_to_openai_function`\n",
    "- ad-hoc scratchpad plumbing (`OpenAIFunctionsAgentOutputParser`, `format_to_openai_functions`, etc.)\n",
    "- `ConversationBufferMemory` (superseded for agents by **LangGraph checkpointers**)\n",
    "\n",
    "In LangChain **1.2.x**, the recommended approach is to use:\n",
    "- `langchain_openai.ChatOpenAI` (provider package)\n",
    "- tools via the `@tool` decorator\n",
    "- graph-based agents via `langchain.agents.create_agent` (built on LangGraph)\n",
    "- short-term memory using a LangGraph **checkpointer** (thread-level persistence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61573e15",
   "metadata": {},
   "source": [
    "## 0) Environment + safety checks\n",
    "\n",
    "This notebook is designed to run in a **Python 3.13** environment with:\n",
    "- `langchain==1.2.0`\n",
    "- `langchain-core==1.2.4`\n",
    "- `langchain-openai==1.1.6`\n",
    "- `langgraph==1.0.5`\n",
    "- `openai==2.14.0`\n",
    "\n",
    "If you *don't* have `OPENAI_API_KEY` set, the notebook will still run end-to-end using a **fake chat model** for deterministic, offline smoke tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2657f30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.11 | packaged by conda-forge | (main, Dec  6 2025, 11:37:04) [Clang 19.1.7 ]\n",
      "✅ Package versions match the expected environment.\n",
      "OPENAI_API_KEY set: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from importlib.metadata import version as pkg_version\n",
    "\n",
    "REQUIRED = {\n",
    "    \"langchain\": \"1.2.0\",\n",
    "    \"langchain-core\": \"1.2.4\",\n",
    "    \"langchain-openai\": \"1.1.6\",\n",
    "    \"langgraph\": \"1.0.5\",\n",
    "    \"openai\": \"2.14.0\",\n",
    "}\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "missing = []\n",
    "mismatched = []\n",
    "\n",
    "for pkg, expected in REQUIRED.items():\n",
    "    try:\n",
    "        got = pkg_version(pkg)\n",
    "        if got != expected:\n",
    "            mismatched.append((pkg, expected, got))\n",
    "    except Exception:\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required packages: {missing}\")\n",
    "\n",
    "if mismatched:\n",
    "    msg = \"\\n\".join([f\"- {pkg}: expected {exp}, got {got}\" for pkg, exp, got in mismatched])\n",
    "    raise RuntimeError(\"Package version mismatch:\\n\" + msg)\n",
    "\n",
    "print(\"✅ Package versions match the expected environment.\")\n",
    "print(\"OPENAI_API_KEY set:\", bool(os.environ.get(\"OPENAI_API_KEY\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6464c8f",
   "metadata": {},
   "source": [
    "## 1) Define tools (2025/2026 tool-calling style)\n",
    "\n",
    "A *tool* is a typed callable the model can invoke.  \n",
    "In 2023, many tutorials converted tools into OpenAI function schemas manually.\n",
    "\n",
    "In LangChain 1.2.x you do **not** do that yourself:\n",
    "- define a tool with `@tool` (schema inferred from type hints), and\n",
    "- pass tools to `create_agent(...)`\n",
    "\n",
    "The agent runtime handles the tool calling protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ad8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import datetime\n",
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> str:\n",
    "    \"\"\"Fetch the current temperature (°C) for a given (lat, lon) using Open-Meteo.\"\"\"\n",
    "    base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"hourly\": \"temperature_2m\",\n",
    "        \"timezone\": \"UTC\",\n",
    "    }\n",
    "\n",
    "    r = requests.get(base_url, params=params, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    # Find the temperature closest to 'now' in UTC\n",
    "    now = datetime.datetime.now(datetime.timezone.utc)\n",
    "    times = [\n",
    "        datetime.datetime.fromisoformat(t).replace(tzinfo=datetime.timezone.utc)\n",
    "        for t in data[\"hourly\"][\"time\"]\n",
    "    ]\n",
    "    temps = data[\"hourly\"][\"temperature_2m\"]\n",
    "\n",
    "    idx = min(range(len(times)), key=lambda i: abs(times[i] - now))\n",
    "    return f\"The current temperature is {temps[idx]}°C (UTC time: {times[idx].isoformat()}).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7cd83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Search Wikipedia and return a short summary for the top result.\"\"\"\n",
    "    try:\n",
    "        title = wikipedia.search(query, results=1)\n",
    "        if not title:\n",
    "            return \"No Wikipedia results found.\"\n",
    "        page = wikipedia.page(title[0], auto_suggest=False)\n",
    "        summary = wikipedia.summary(page.title, sentences=3, auto_suggest=False)\n",
    "        return f\"Title: {page.title}\\nSummary: {summary}\"\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return f\"Your query is ambiguous. Options include: {', '.join(e.options[:8])}...\"\n",
    "    except Exception as e:\n",
    "        return f\"Wikipedia error: {type(e).__name__}: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1334835",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def create_your_own(query: str) -> str:\n",
    "    \"\"\"Example custom tool: reverse the user's string.\"\"\"\n",
    "    return query[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf461619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get_current_temperature', 'search_wikipedia', 'create_your_own']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [get_current_temperature, search_wikipedia, create_your_own]\n",
    "[t.name for t in tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cfaced",
   "metadata": {},
   "source": [
    "## 2) Create a modern agent (LangGraph-powered)\n",
    "\n",
    "In LangChain 1.2.x, the recommended agent entry point is:\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "```\n",
    "\n",
    "This returns a **graph runnable** (LangGraph under the hood).  \n",
    "For multi-turn conversation, use a **checkpointer** and provide a `thread_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "212dab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# ---- model selection ----\n",
    "# If no OPENAI_API_KEY is present, we fall back to a deterministic fake chat model\n",
    "# so the notebook still runs end-to-end.\n",
    "def get_model():\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if api_key:\n",
    "        return ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n",
    "    # Offline: deterministic fake model for smoke testing\n",
    "    from langchain_core.language_models.fake_chat_models import FakeListChatModel\n",
    "    from langchain_core.messages import AIMessage\n",
    "    return FakeListChatModel(responses=[\n",
    "        AIMessage(content=\"(offline demo) I can't call external tools without a real model, but the agent wiring works.\")\n",
    "    ])\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "memory = InMemorySaver()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are helpful but a little sassy. Be concise.\",\n",
    "    checkpointer=memory,\n",
    ")\n",
    "\n",
    "# Each conversation thread is identified by thread_id.\n",
    "CONFIG = {\"configurable\": {\"thread_id\": \"demo-thread-1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a26f1",
   "metadata": {},
   "source": [
    "## 3) Invoke the agent\n",
    "\n",
    "The agent expects a state update containing messages.\n",
    "\n",
    "We pass:\n",
    "```python\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"...\"}]}\n",
    "```\n",
    "\n",
    "and (when using memory) a config containing:\n",
    "```python\n",
    "{\"configurable\": {\"thread_id\": \"...\"}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94770fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Short answer: LangChain is an open-source framework for building applications that use large language models (LLMs). It wraps LLMs with tools, memory, and orchestration so you can build more capable, reliable, and data-aware apps than \"single-prompt → response.\"\\n\\nWhat it does (high level)\\n- Connects LLMs to data, APIs, and tools (databases, Google Drive, search, calculators, web scraping, etc.).\\n- Provides building blocks to compose multi-step \"chains\" of prompts and logic.\\n- Adds memory and state so conversations/apps can be context-aware.\\n- Offers \"agents\" that let models decide which tools to call and when.\\n\\nCore concepts (brief)\\n- LLM wrappers: standard interface to different models (OpenAI, local models, Hugging Face, etc.).\\n- Prompts & prompt templates: reusable, parameterized prompts.\\n- Chains: sequences of operations (prompts, transformations, tool calls).\\n- Agents & Tools: let the model choose and execute external actions (e.g., run a search, call an API).\\n- Memory: short- and long-term context storage for multi-turn interactions.\\n- Connectors: prebuilt integrations to data stores and APIs.\\n- LangChain Hub: shareable components (prompts, chains, tools).\\n\\nCommon use cases\\n- Conversational agents / chatbots that use external data.\\n- Retrieval-augmented generation (RAG) — answer questions from your documents.\\n- Workflow automation where the model calls services or APIs.\\n- Data extraction, summarization, and question-answering over corpora.\\n\\nPros\\n- Speeds up building production-ready LLM apps.\\n- Modular and extensible.\\n- Wide ecosystem of connectors and community components.\\n\\nLimitations / risks\\n- Adds complexity — might be overkill for simple one-shot prompts.\\n- Safety, hallucination, and privacy issues still apply; you must validate tool outputs and manage sensitive data.\\n- Performance and cost depend on model choices and architecture.\\n\\nTiny Python-esque example (conceptual)\\n- Create an LLM -> create a retriever over docs -> make a chain that retrieves + prompts -> run query.\\nIf you want, I can give an exact runnable Python or JavaScript example.\\n\\nWant a short runnable example (Python or JS)? Which model/connectors do you want to use?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask(user_text: str):\n",
    "    result = agent.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": user_text}]},\n",
    "        CONFIG,\n",
    "    )\n",
    "    # Agent returns a state dict with 'messages'\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "ask(\"What is LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ec6f5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Bob — I’ll remember that for this chat. What can I help you with?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"My name is Bob.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cbaa0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name's Bob. Got it — I’ll remember that for this chat.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"What's my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df290774",
   "metadata": {},
   "source": [
    "## 4) Tool-using prompts (weather + Wikipedia)\n",
    "\n",
    "If you have a valid `OPENAI_API_KEY`, the agent can decide to call tools.\n",
    "\n",
    "Example prompts:\n",
    "- \"What's the weather at lat 37.7749, lon -122.4194?\"\n",
    "- \"Wikipedia: what is LangChain?\"\n",
    "\n",
    "If you're running offline (no API key), you'll still get a clean response without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0edda2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature at latitude 37.7749, longitude -122.4194 is 12.0°C.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"What's the weather at latitude 37.7749 and longitude -122.4194?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d28e80bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From Wikipedia: LangChain is a software framework that helps integrate large language models (LLMs) into applications. It supports use cases like document analysis, summarization, chatbots, and code analysis. It was launched in October 2022 by Harrison Chase while at Robust Intelligence.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"Wikipedia: what is LangChain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a46a5",
   "metadata": {},
   "source": [
    "## 5) A tiny chat loop (no extra UI dependencies)\n",
    "\n",
    "The original 2023 notebook used `panel` for a GUI.  \n",
    "This environment does **not** require any GUI libs: run a minimal terminal-style loop instead.\n",
    "\n",
    "Stop with an empty input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd063b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a message (empty to stop).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you>  what is the temperature in New York City\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot> The current temperature in New York City (lat 40.7128, lon -74.0060) is 1.3°C.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you>  Wikipedia: what is the hibert hotel? And why don't real numbers fit in that hotel?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot> You mean Hilbert’s Hotel — a famous thought experiment (from Wikipedia and elsewhere) by David Hilbert illustrating strange properties of infinite sets.\n",
      "\n",
      "What Hilbert’s Hotel is\n",
      "- Imagine a hotel with rooms numbered 1, 2, 3, … (one room for every natural number). It’s full: every room has a guest.\n",
      "- Despite being “full,” it can still accommodate more guests by reassigning rooms. For example:\n",
      "  - To fit one more guest, move the guest in room n to room n+1; room 1 becomes free.\n",
      "  - To fit countably infinitely many new guests, move the guest in room n to room 2n; all odd-numbered rooms become free for the newcomers.\n",
      "- This shows counterintuitive behavior of countably infinite sets: they can be put in one-to-one correspondence with proper subsets of themselves.\n",
      "\n",
      "Why the real numbers don’t fit\n",
      "- The hotel’s rooms correspond to the natural numbers, so it can only host sets that are countable (i.e., can be listed as a sequence r1, r2, r3, …).\n",
      "- The real numbers are uncountable: there is no way to list all real numbers in a sequence that hits every real.\n",
      "- Cantor’s diagonal argument (sketch): assume you could list all real numbers between 0 and 1 as r1, r2, r3, … and write each in decimal form. Construct a new number whose nth decimal differs from the nth decimal of rn. That new number cannot equal any rn, contradicting the assumption that the list was complete. Hence, the reals can’t be put into one-to-one correspondence with the naturals.\n",
      "- Conclusion: Hilbert’s Hotel can host any countably infinite set (like integers or rationals) but cannot host the uncountable set of real numbers — there simply aren’t enough “room numbers” to enumerate them.\n",
      "\n",
      "Short version: Hilbert’s Hotel works for countable infinities; the real numbers are a strictly larger (uncountable) infinity, so they won’t fit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you>  what is the significance of the reals can’t be put into one-to-one correspondence with the naturals.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot> Short answer: it shows there are different “sizes” of infinity — the reals form a strictly larger infinite set than the naturals — and that has deep consequences for math, logic, and computation.\n",
      "\n",
      "Why that matters (concise points)\n",
      "- Different cardinalities: |N| (countable) < |R| (uncountable). The reals have cardinality 2^{aleph0} (the continuum), not aleph0. This breaks the naive idea that all infinities are the same.\n",
      "- Cantor’s diagonal method: the proof technique that shows uncountability is simple but powerful; it underpins many impossibility results across math and CS.\n",
      "- Hierarchy of infinities: leads to a whole stratified universe of larger and larger infinite cardinalities (power sets give strictly bigger sizes).\n",
      "- Foundations & set theory: the Continuum Hypothesis (is there a size strictly between |N| and |R|?) becomes a central, delicate question — it is independent of the usual axioms of set theory (ZFC).\n",
      "- Computability and information: most real numbers are uncomputable (cannot be produced by any algorithm). So “almost all” reals can’t be described finitely or generated by programs.\n",
      "- Algebra & number theory: the algebraic numbers are countable, so almost every real number is transcendental.\n",
      "- Analysis & topology: uncountability underlies the continuum properties used in calculus, measure theory, and topology (e.g., intervals containing uncountably many points, nontrivial Lebesgue measure behavior).\n",
      "- Practical/Philosophical: it affects how we model reality — mathematicians treat continua as uncountable, but philosophers and some physicists ask whether the physical world might be discrete.\n",
      "\n",
      "If you want, I can:\n",
      "- Walk through Cantor’s diagonal proof step-by-step.\n",
      "- Explain the Continuum Hypothesis and its independence.\n",
      "- Show why most reals are uncomputable or transcendental.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you>  Show why most reals are uncomputable or transcendental.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot> Short answer: because the sets of algorithms (programs) and of algebraic numbers are both countable, while the real numbers are uncountable — so almost every real number (in the sense of cardinality and Lebesgue measure) is neither produced by any algorithm nor a root of any integer-coefficient polynomial.\n",
      "\n",
      "Proof sketches (concise)\n",
      "\n",
      "1) Most reals are uncomputable\n",
      "- A computable real is one for which some finite program outputs arbitrarily good rational approximations.\n",
      "- Programs are finite strings over a finite alphabet, so the set of all programs is countable.\n",
      "- Each program can define at most one real number, so the set of computable reals is countable.\n",
      "- The real numbers are uncountable (Cantor), so there are uncountably many reals that aren’t computable. Hence “most” reals are uncomputable.\n",
      "\n",
      "2) Most reals are transcendental\n",
      "- An algebraic number is a root of some nonzero polynomial with integer coefficients.\n",
      "- For each fixed degree n, the set of integer-coefficient polynomials of degree ≤ n is countable (Z^{n+1} is countable), and each such polynomial has at most n roots.\n",
      "- Thus the set of algebraic numbers is a countable union of finite sets, hence countable.\n",
      "- Since R is uncountable, there are uncountably many reals that are not algebraic — these are transcendental. So “most” reals are transcendental.\n",
      "\n",
      "3) Measure-theoretic strengthening\n",
      "- Any countable subset of R has Lebesgue measure zero (cover the nth point by an interval of length ε/2^n; total length ≤ ε, and ε is arbitrary).\n",
      "- Therefore the sets of computable reals and algebraic reals both have measure zero.\n",
      "- The complement therefore has full measure: almost every real number (in the sense of Lebesgue measure) is both uncomputable and transcendental.\n",
      "\n",
      "Important clarifications\n",
      "- “Most” here is formal: countably many vs. uncountably many, and measure zero vs. full measure.\n",
      "- Computable ↔/↔ transcendental: these properties are independent. There are computable transcendental numbers (e.g., e, π, Liouville’s number) and all algebraic numbers are in fact computable. So being transcendental doesn’t imply uncomputable, and being computable doesn’t imply algebraic. Still, the overwhelming majority of reals are both transcendental and uncomputable.\n",
      "\n",
      "Want a short example showing how to list all programs (so the countability argument feels concrete) or a quick construction of a specific uncomputable real?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you>  wikipedia: Walk through Cantor’s diagonal proof step-by-step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot> Sure — let’s walk through Cantor’s diagonal argument step by step. I’ll use decimal expansions in the interval (0,1) (the same idea works in binary and for other sets). I’ll point out the little technical wrinkle about repeating 9s and how to avoid it.\n",
      "\n",
      "Goal: show that the real numbers in (0,1) cannot be listed as r1, r2, r3, … (i.e., they are uncountable).\n",
      "\n",
      "1. Assume the opposite (for contradiction).\n",
      "   - Suppose every real in (0,1) appears somewhere in an infinite list:\n",
      "     r1, r2, r3, …\n",
      "\n",
      "2. Write each rn in decimal form.\n",
      "   - rn = 0.dn1 dn2 dn3 … where dnk is the k-th decimal digit of rn.\n",
      "   - To avoid ambiguity (e.g., 0.4999… = 0.5000…), choose for each real its decimal expansion that does NOT end in an infinite string of 9s. This is always possible.\n",
      "\n",
      "3. Construct a new number x by changing the diagonal digits.\n",
      "   - Define x = 0.x1 x2 x3 … where each xn is chosen so that xn ≠ dnn.\n",
      "   - For instance, set xn = 5 if dnn ≠ 5, and xn = 4 if dnn = 5. (Any rule that guarantees xn differs from dnn works; avoid creating an infinite tail of 9s with your rule, but the simple choice above is safe.)\n",
      "\n",
      "4. Check that x is not in the list.\n",
      "   - By construction, x differs from r1 in the 1st decimal place, differs from r2 in the 2nd decimal place, differs from r3 in the 3rd decimal place, and so on.\n",
      "   - Therefore x ≠ rn for every n. So x does not appear anywhere in the supposed complete list.\n",
      "\n",
      "5. Contradiction and conclusion.\n",
      "   - We assumed the list contained every real in (0,1). We found a real x that’s not in the list. Contradiction.\n",
      "   - Hence no such list exists: the reals in (0,1) are uncountable.\n",
      "\n",
      "Why this matters (brief):\n",
      "- The diagonal trick shows you can always build an element that evades any purported listing by changing the diagonal entries.\n",
      "- The argument generalizes: the power set of any set has strictly larger cardinality than the set itself (Cantor’s theorem), and diagonal methods show up in many impossibility results (e.g., uncomputability proofs).\n",
      "\n",
      "Want to see the binary version (cleaner because of fewer ambiguous expansions) or the formal statement about power sets?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you>  \n"
     ]
    }
   ],
   "source": [
    "def chat():\n",
    "    print(\"Type a message (empty to stop).\")\n",
    "    while True:\n",
    "        user = input(\"you> \").strip()\n",
    "        if not user:\n",
    "            break\n",
    "        print(\"bot>\", ask(user))\n",
    "\n",
    "# Uncomment to use interactively in a local Jupyter session:\n",
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

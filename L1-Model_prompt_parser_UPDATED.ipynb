{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47d10e6",
   "metadata": {},
   "source": [
    "# L1 — Model, Prompts, and Output Parsing (LangChain v1 + OpenAI Responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12f2a8",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "This notebook uses **OpenAI (Python SDK v2) + LangChain v1**.\n",
    "\n",
    "## Prereqs\n",
    "1. Set your API key in the environment:\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"...\"\n",
    "```\n",
    "\n",
    "2. Restart the kernel after setting env vars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b294398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make sure your key is set\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Set OPENAI_API_KEY in your environment before running.\"\n",
    "\n",
    "MODEL = \"gpt-5-mini\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212a6d6",
   "metadata": {},
   "source": [
    "## 1) Direct OpenAI call (Responses API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc3bf7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "resp = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=\"What is 1+1? Answer with just the number.\"\n",
    ")\n",
    "\n",
    "print(resp.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb01c4",
   "metadata": {},
   "source": [
    "## 2) LangChain ChatOpenAI + Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9525ff83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 1 = 2 because adding one unit to another unit results in two units.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "msg = chain.invoke({\"question\": \"Explain 1+1 in one sentence.\"})\n",
    "print(msg.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5557b6",
   "metadata": {},
   "source": [
    "## 3) Structured output with Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e63cfce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parser is an object of type: <class 'langchain_core.output_parsers.pydantic.PydanticOutputParser'>\n",
      "prompt is an object of type: <class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "chain is an object of type: <class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "out is an object of type: <class '__main__.MathAnswer'>\n",
      "answer=2 explanation='1 plus 1 equals 2.'\n",
      "2 1 plus 1 equals 2.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class MathAnswer(BaseModel):\n",
    "    answer: int = Field(..., description=\"The numeric answer.\")\n",
    "    explanation: str = Field(..., description=\"Short explanation.\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MathAnswer)\n",
    "print(f'parser is an object of type: {type(parser)}')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Return JSON only. Follow the format instructions strictly.{format_instructions}\"),\n",
    "    (\"user\", \"Question: {question}\")\n",
    "]).partial(format_instructions=parser.get_format_instructions())\n",
    "print(f'prompt is an object of type: {type(prompt)}')\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "print(f'chain is an object of type: {type(chain)}')\n",
    "\n",
    "out = chain.invoke({\"question\": \"What is 1+1?\"})\n",
    "print(f'out is an object of type: {type(out)}')\n",
    "print(out)\n",
    "print(out.answer, out.explanation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e08926",
   "metadata": {},
   "source": [
    "## 4) Tool-free 'prompt parser' pattern (robust extraction)\n",
    "If the model returns extra text, parse with a schema and retry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01977cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer=12 explanation='7 + 5 = 12.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def robust_parse(question: str, max_tries: int = 2):\n",
    "    last_err = None\n",
    "    for _ in range(max_tries):\n",
    "        try:\n",
    "            return chain.invoke({\"question\": question})\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "print(robust_parse(\"What is 7+5?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2aba61-3d65-4935-b96a-f4d1fbcd6342",
   "metadata": {},
   "source": [
    "## 5) Best of Both Worlds - Structured Output + Retries + \"Automatic Fix the JSON and Retry” Behavior When Parsing Fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a54caf-7fc8-4f1b-8bc4-aee97c4599e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer=12 explanation='7 + 5 equals 12 because adding seven and five yields twelve.'\n",
      "12\n",
      "7 + 5 equals 12 because adding seven and five yields twelve.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_classic.output_parsers import OutputFixingParser\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "\n",
    "# 1) Define the schema you want back\n",
    "class MathAnswer(BaseModel):\n",
    "    answer: int = Field(..., description=\"The numeric answer.\")\n",
    "    explanation: str = Field(..., description=\"Short explanation.\")\n",
    "\n",
    "# 2) Build an LLM\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0)\n",
    "\n",
    "# 3) Base parser (strict)\n",
    "base_parser = PydanticOutputParser(pydantic_object=MathAnswer)\n",
    "\n",
    "# 4) Fixing parser (if parsing fails, it uses the LLM to fix the output)\n",
    "fixing_parser = OutputFixingParser.from_llm(llm=llm, parser=base_parser)\n",
    "\n",
    "# 5) Prompt that tells the model to output JSON matching the schema\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Return JSON only. Follow the format instructions strictly.\\n{format_instructions}\"),\n",
    "    (\"user\", \"Question: {question}\")\n",
    "]).partial(format_instructions=base_parser.get_format_instructions())\n",
    "\n",
    "# 6) IMPORTANT: call the LLM first, then parse with \"try strict, then fix\"\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "def invoke_structured(question: str, max_tries: int = 3) -> MathAnswer:\n",
    "    last_err = None\n",
    "\n",
    "    for _ in range(max_tries):\n",
    "        try:\n",
    "            msg = llm_chain.invoke({\"question\": question})\n",
    "            text = msg.content\n",
    "\n",
    "            # Try strict parsing first\n",
    "            try:\n",
    "                return base_parser.parse(text)\n",
    "            except OutputParserException:\n",
    "                # If it's not valid JSON / wrong schema, ask LLM to \"fix\" it and re-parse\n",
    "                return fixing_parser.parse(text)\n",
    "\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "\n",
    "    raise last_err\n",
    "\n",
    "# Example\n",
    "out = invoke_structured(\"What is 7+5?\")\n",
    "print(out)                 # Pydantic model repr (not JSON)\n",
    "print(out.answer)          # 12\n",
    "print(out.explanation)     # short explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e1192b-6c86-4e96-acd8-064293da26b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "title": "L1-Model_prompt_parser_UPDATED.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

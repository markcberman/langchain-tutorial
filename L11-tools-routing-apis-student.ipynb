{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b815ef73",
   "metadata": {},
   "source": [
    "# Tools and Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb41f5f4-df8d-4d04-9eaa-193b8c29b00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139c45e6-65a4-4fcb-a109-e5ba41a80835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a17f623-9342-46bd-a913-9ea6be32a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for weather online\"\"\"\n",
    "    return \"42f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db510949-acb8-4f69-b6c9-242b84fdc63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c12426-5e1a-4057-8ae5-6dfb6beace97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Search for weather online'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22d1c4e6-8313-41a0-ae7a-61343699cb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'title': 'Query', 'type': 'string'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8765af16-17fc-4490-98ab-4e9dd59337cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"Thing to search for\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d20261-e18f-4989-808c-c4f36643d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(args_schema=SearchInput)\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for the weather online.\"\"\"\n",
    "    return \"42f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b3f7d0-a856-4842-82da-6c00df53e9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'description': 'Thing to search for',\n",
       "  'title': 'Query',\n",
       "  'type': 'string'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060dc15b-f95f-47ab-b081-d6736f277ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42f'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.run(\"sf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e93b85b2-1786-4f63-8e65-cd6a8b5c0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    import requests\n",
    "    from datetime import datetime, UTC\n",
    "\n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"hourly\": \"temperature_2m\",\n",
    "        \"forecast_days\": 1,\n",
    "        \"timezone\": \"UTC\",  # helps make returned times consistent\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    response.raise_for_status()\n",
    "    results = response.json()\n",
    "\n",
    "    current_utc_time = datetime.now(UTC)\n",
    "\n",
    "    def _parse_to_utc(ts: str) -> datetime:\n",
    "        # Handle \"Z\" suffix if present\n",
    "        dt = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "        # If Open-Meteo returned a naive timestamp, assume it's UTC\n",
    "        if dt.tzinfo is None:\n",
    "            return dt.replace(tzinfo=UTC)\n",
    "        # Otherwise normalize to UTC\n",
    "        return dt.astimezone(UTC)\n",
    "\n",
    "    time_list = [_parse_to_utc(t) for t in results[\"hourly\"][\"time\"]]\n",
    "    temperature_list = results[\"hourly\"][\"temperature_2m\"]\n",
    "\n",
    "    closest_time_index = min(\n",
    "        range(len(time_list)),\n",
    "        key=lambda i: abs(time_list[i] - current_utc_time),\n",
    "    )\n",
    "\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    return f\"The current temperature is {current_temperature}°C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "731ed353-a27d-42df-89a8-9767bafb23be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d4f62eb8-c528-4e4a-b078-22e9d9b20a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fetch current temperature for given coordinates.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "38ebb59f-cf0f-4281-99c3-87c8cfd3377b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': {'description': 'Latitude of the location to fetch weather data for',\n",
       "  'title': 'Latitude',\n",
       "  'type': 'number'},\n",
       " 'longitude': {'description': 'Longitude of the location to fetch weather data for',\n",
       "  'title': 'Longitude',\n",
       "  'type': 'number'}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "36aacbec-9cdd-414a-b502-168cea350a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6277b6a8-f197-4057-a96a-5ebb61e18520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'get_current_temperature',\n",
       "  'description': 'Fetch current temperature for given coordinates.',\n",
       "  'parameters': {'properties': {'latitude': {'description': 'Latitude of the location to fetch weather data for',\n",
       "     'type': 'number'},\n",
       "    'longitude': {'description': 'Longitude of the location to fetch weather data for',\n",
       "     'type': 'number'}},\n",
       "   'required': ['latitude', 'longitude'],\n",
       "   'type': 'object'}}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_openai_tool(get_current_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e088eceb-424b-4da6-981f-63322c9ac56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature is 21.2°C'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature.invoke({\"latitude\": 13, \"longitude\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5eefc272-d205-4bad-ab00-282ccb3cc2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    import wikipedia\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            self.wiki_client.exceptions.PageError,\n",
    "            self.wiki_client.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "289c5d0c-0e71-4d70-963f-9d9f58e53d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search_wikipedia'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "69756d79-ec2f-4185-89af-d14de817fc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run Wikipedia search and get page summaries.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d5b876cd-cc6c-484d-a3fa-d8810e22d564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'search_wikipedia',\n",
       "  'description': 'Run Wikipedia search and get page summaries.',\n",
       "  'parameters': {'properties': {'query': {'type': 'string'}},\n",
       "   'required': ['query'],\n",
       "   'type': 'object'}}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_openai_tool(search_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6dfb10e9-5d1e-474a-9c8f-7abdf7939a34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: LangChain\\nSummary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\\n\\nPage: Vector database\\nSummary: A vector database, vector store or vector search engine is a database that stores and retrieves embeddings of data in vector space. Vector databases typically implement approximate nearest neighbor algorithms so users can search for records semantically similar to a given input, unlike traditional databases which primarily look up records by exact match. Use-cases for vector databases include similarity search, semantic search, multi-modal search, recommendations engines, object detection, and retrieval-augmented generation (RAG).\\nVector embeddings are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. Each data item is represented by one vector in this space. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\\nThese feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\\n\\nPage: Retrieval-augmented generation\\nSummary: Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information from external data sources. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM\\'s pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don\\'t exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.invoke({\"query\": \"langchain\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016ff34-bcdf-4ad3-98b6-da3917e29c92",
   "metadata": {},
   "source": [
    "## REQUIRES CONVERTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "54cbe57e-f64c-4392-bcdd-9621fe1e46e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_community.agent_toolkits.json.toolkit import JsonToolkit\n",
    "from langchain_community.agent_toolkits.openapi.toolkit import RequestsToolkit\n",
    "from langchain_community.tools.json.tool import JsonSpec\n",
    "from langchain_community.utilities.requests import TextRequestsWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7fe38e50-874c-49bd-8681-f95dcab0b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "{\n",
    "  \"openapi\": \"3.0.0\",\n",
    "  \"info\": {\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"title\": \"Swagger Petstore\",\n",
    "    \"license\": {\n",
    "      \"name\": \"MIT\"\n",
    "    }\n",
    "  },\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"url\": \"http://petstore.swagger.io/v1\"\n",
    "    }\n",
    "  ],\n",
    "  \"paths\": {\n",
    "    \"/pets\": {\n",
    "      \"get\": {\n",
    "        \"summary\": \"List all pets\",\n",
    "        \"operationId\": \"listPets\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"parameters\": [\n",
    "          {\n",
    "            \"name\": \"limit\",\n",
    "            \"in\": \"query\",\n",
    "            \"description\": \"How many items to return at one time (max 100)\",\n",
    "            \"required\": false,\n",
    "            \"schema\": {\n",
    "              \"type\": \"integer\",\n",
    "              \"maximum\": 100,\n",
    "              \"format\": \"int32\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"200\": {\n",
    "            \"description\": \"A paged array of pets\",\n",
    "            \"headers\": {\n",
    "              \"x-next\": {\n",
    "                \"description\": \"A link to the next page of responses\",\n",
    "                \"schema\": {\n",
    "                  \"type\": \"string\"\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Pets\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"post\": {\n",
    "        \"summary\": \"Create a pet\",\n",
    "        \"operationId\": \"createPets\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"201\": {\n",
    "            \"description\": \"Null response\"\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"/pets/{petId}\": {\n",
    "      \"get\": {\n",
    "        \"summary\": \"Info for a specific pet\",\n",
    "        \"operationId\": \"showPetById\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"parameters\": [\n",
    "          {\n",
    "            \"name\": \"petId\",\n",
    "            \"in\": \"path\",\n",
    "            \"required\": true,\n",
    "            \"description\": \"The id of the pet to retrieve\",\n",
    "            \"schema\": {\n",
    "              \"type\": \"string\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"200\": {\n",
    "            \"description\": \"Expected response to a valid request\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Pet\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"components\": {\n",
    "    \"schemas\": {\n",
    "      \"Pet\": {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\n",
    "          \"id\",\n",
    "          \"name\"\n",
    "        ],\n",
    "        \"properties\": {\n",
    "          \"id\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"format\": \"int64\"\n",
    "          },\n",
    "          \"name\": {\n",
    "            \"type\": \"string\"\n",
    "          },\n",
    "          \"tag\": {\n",
    "            \"type\": \"string\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"Pets\": {\n",
    "        \"type\": \"array\",\n",
    "        \"maxItems\": 100,\n",
    "        \"items\": {\n",
    "          \"$ref\": \"#/components/schemas/Pet\"\n",
    "        }\n",
    "      },\n",
    "      \"Error\": {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\n",
    "          \"code\",\n",
    "          \"message\"\n",
    "        ],\n",
    "        \"properties\": {\n",
    "          \"code\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"format\": \"int32\"\n",
    "          },\n",
    "          \"message\": {\n",
    "            \"type\": \"string\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "653156d4-d6a4-499b-aab3-cf4f7b1f0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the OpenAPI spec (as JSON here) and wrap it in a JsonSpec for the toolkit.\n",
    "spec_dict = json.loads(text)\n",
    "json_spec = JsonSpec(dict_=spec_dict, max_value_length=4000)\n",
    "\n",
    "# Requests wrapper (no auth needed for the Petstore example).\n",
    "requests_wrapper = TextRequestsWrapper(headers={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d9ba6100-3c88-4b05-af93-784b57bf7424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " ['requests_get',\n",
       "  'requests_post',\n",
       "  'requests_patch',\n",
       "  'requests_put',\n",
       "  'requests_delete',\n",
       "  'json_spec_list_keys',\n",
       "  'json_spec_get_value']]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a modern set of Tools for interacting with an OpenAPI-described API.\n",
    "#\n",
    "# NOTE: OpenAPIToolkit.get_tools() includes a `json_explorer` tool implemented as an *agent*.\n",
    "# That agent internally uses stop sequences, which GPT-5-mini rejects (no `stop` support).\n",
    "# To stay fully modern (tools-first) *and* compatible with GPT-5-mini, we compose:\n",
    "#   - RequestsToolkit: deterministic HTTP request tools (GET/POST/PATCH/PUT/DELETE)\n",
    "#   - JsonToolkit: deterministic tools for exploring the OpenAPI JSON spec\n",
    "# This avoids any legacy agent/function-calling shims and avoids passing `stop`.\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", disabled_params={\"stop\": None})\n",
    "\n",
    "requests_toolkit = RequestsToolkit(\n",
    "    requests_wrapper=requests_wrapper,\n",
    "    allow_dangerous_requests=True,\n",
    ")\n",
    "json_toolkit = JsonToolkit(spec=json_spec)\n",
    "\n",
    "openapi_tools = [*requests_toolkit.get_tools(), *json_toolkit.get_tools()]\n",
    "[len(openapi_tools), [t.name for t in openapi_tools]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "91199697-c662-438f-8c68-e6daa8aad07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind the OpenAPI tools to the model (OpenAI \"tools\" schema).\n",
    "llm_with_openapi_tools = llm.bind_tools(openapi_tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ae8b3c1b-7a30-4b1b-abfd-e56f90b1d166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are three pet names: Bella, Max, Luna.\\n\\nWant names for a specific type (dog, cat, bird) or personality?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 706, 'total_tokens': 807, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Ctj6p9BHTmSokKljRABR4U3etmftS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8119-4009-7562-be6a-b43bf8f937fc-0', usage_metadata={'input_tokens': 706, 'output_tokens': 101, 'total_tokens': 807, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_openapi_tools.invoke(\"What are three pet names? (Call the API if you need to.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ed100888-f3d4-4739-bba5-f839033850a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 706, 'total_tokens': 804, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Ctj6sG7Y4dwEJBwrDuOZTdIOdJxUv', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b8119-50ed-7fa2-be6a-55858d1c81fe-0', tool_calls=[{'name': 'requests_get', 'args': {'url': 'https://petstore.swagger.io/v2/pet/42'}, 'id': 'call_j4vZwzz28FZbQKomLZg1Q6Uw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 706, 'output_tokens': 98, 'total_tokens': 804, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_openapi_tools.invoke(\"Tell me about the pet with id 42. (Use the API.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4063cdea-627c-4cc1-a2b5-9b5b6dcf179e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'requests_get',\n",
       "  'args': {'url': 'https://petstore.swagger.io/v2/pet/42'},\n",
       "  'id': 'call_O2f9YzZVrj4ljoGUDlSUTq7h',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the model chose to call a tool, you'll see tool_calls on the AIMessage.\n",
    "msg = llm_with_openapi_tools.invoke(\"Look up the pet with id 42 and summarize it in one sentence.\")\n",
    "msg.tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3a1f5534-cf06-4baf-b710-f169ea7434ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolMessage(content='{\"code\":1,\"type\":\"error\",\"message\":\"Pet not found\"}', tool_call_id='call_O2f9YzZVrj4ljoGUDlSUTq7h')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper: execute tool calls locally (single-step execution).\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "def execute_tool_calls(ai_message: AIMessage, tools):\n",
    "    tool_map = {t.name: t for t in tools}\n",
    "    tool_messages = []\n",
    "    for call in ai_message.tool_calls or []:\n",
    "        name = call[\"name\"]\n",
    "        args = call.get(\"args\") or {}\n",
    "        result = tool_map[name].invoke(args)\n",
    "        tool_messages.append(ToolMessage(content=str(result), tool_call_id=call[\"id\"]))\n",
    "    return tool_messages\n",
    "\n",
    "tool_messages = execute_tool_calls(msg, openapi_tools)\n",
    "tool_messages[:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b21dd-c9de-491d-9a0c-71ae56727689",
   "metadata": {},
   "source": [
    "### Routing (modern tool-calling)\n",
    "\n",
    "In lesson 3, we showed an example of **function calling** deciding between two candidate functions.\n",
    "\n",
    "In LangChain v1 (and OpenAI's current API), the modern pattern is **tool calling**:\n",
    "- You bind tools with `llm.bind_tools([...])`\n",
    "- The model returns `AIMessage.tool_calls`\n",
    "- Your application executes the tool(s) and optionally feeds results back to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8137758c-c5d3-47df-9062-5e31d43657e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'search_wikipedia',\n",
       "   'description': 'Run Wikipedia search and get page summaries.',\n",
       "   'parameters': {'properties': {'query': {'type': 'string'}},\n",
       "    'required': ['query'],\n",
       "    'type': 'object'}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'get_current_temperature',\n",
       "   'description': 'Fetch current temperature for given coordinates.',\n",
       "   'parameters': {'properties': {'latitude': {'description': 'Latitude of the location to fetch weather data for',\n",
       "      'type': 'number'},\n",
       "     'longitude': {'description': 'Longitude of the location to fetch weather data for',\n",
       "      'type': 'number'}},\n",
       "    'required': ['latitude', 'longitude'],\n",
       "    'type': 'object'}}}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "tools = [search_wikipedia, get_current_temperature]\n",
    "openai_tools_schema = [convert_to_openai_tool(t) for t in tools]\n",
    "openai_tools_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "99120542-36bc-4ed1-aa9e-e2294e282c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, disabled_params={\"stop\": None}).bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a20047d9-4b34-4e6c-9407-570af1559bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 184, 'total_tokens': 283, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Ctj6yOXzix1oRI97PBp6XiKqiN0op', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b8119-66b4-7f22-bcff-831e695eadde-0', tool_calls=[{'name': 'get_current_temperature', 'args': {'latitude': 37.7749, 'longitude': -122.4194}, 'id': 'call_0yW1wo0YVWMfEUGqA5eDF2jT', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 99, 'total_tokens': 283, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the weather in San Francisco right now?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "831a0506-1d5c-4bc3-b67f-24e201a4fa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Short answer\\n- LangChain is an open-source framework for building applications that use large language models (LLMs). It provides reusable components and patterns so you can combine LLMs with data, tools, and logic to build production-ready apps (chatbots, RAG systems, agents, summarizers, etc.).\\n\\nCore ideas and components\\n- Chains: Composable pipelines that connect prompts, LLM calls, parsing, and other steps into workflows.\\n- Prompts & PromptTemplates: Structured prompt management (templates, partials, formatting helpers).\\n- Models & Adapters: Unified interface for calling many LLM providers (OpenAI, Anthropic, Hugging Face, local models, etc.).\\n- Agents & Tools: Agents plan actions and call external tools or APIs dynamically (search, calculators, file loaders, custom functions).\\n- Retrievers & Vector Stores: Retrieval layers for RAG — nearest-neighbor search over embeddings with connectors to stores like FAISS, Pinecone, Milvus, Weaviate, Chroma, etc.\\n- Memory: Stateful components for maintaining conversation context across turns (short-term and long-term options).\\n- Document Loaders & Text Splitters: Connectors to ingest documents from files, web pages, databases and chunk them for retrieval.\\n- Utilities: Callbacks, caching, output parsers, evaluation helpers, and orchestration primitives.\\n\\nCommon use cases\\n- Retrieval-Augmented Generation (RAG) for grounding answers in your documents or knowledge base.\\n- Conversational assistants that maintain context and call external tools.\\n- Autonomous agents that plan multi-step tasks and use tools/APIs.\\n- Summarization, Q&A, code assistants, data extraction, and content generation pipelines.\\n\\nExample (minimal Python-style pseudo code)\\n- Compose an LLM prompt and run it:\\n  from langchain import OpenAI, LLMChain, PromptTemplate\\n  template = \"Summarize this text in one sentence:\\\\n\\\\n{text}\"\\n  prompt = PromptTemplate(template=template, input_variables=[\"text\"])\\n  llm = OpenAI(temperature=0)\\n  chain = LLMChain(llm=llm, prompt=prompt)\\n  result = chain.run({\"text\": \"Long article content...\"})\\n\\nExample (retrieval + QA, high level)\\n  - load docs -> create embeddings -> store in vector DB -> use a Retriever + LLM to answer user questions based on retrieved context.\\n\\nAPI surfaces\\n- LangChain has both Python and JavaScript/TypeScript SDKs and many third-party integrations. It provides high-level primitives so you don’t handle low-level model I/O, embeddings, or vector-store plumbing yourself.\\n\\nWhen to use LangChain\\n- Use it when you need to: combine LLMs with external data or tools, build multi-step or stateful LLM workflows, and rapidly prototype production-ready LLM apps with standard best-practice components.\\n\\nWhen not to use it\\n- For tiny one-off scripts that only call an LLM once with a simple prompt, a direct API call may suffice; LangChain adds structure and dependencies that are most useful for more complex apps.\\n\\nWhere to learn more\\n- Official docs and GitHub repo have tutorials, examples, and integration docs. If you want, I can:\\n  - give a runnable example for Python or JavaScript\\n  - show how to build a RAG chatbot with a specific vector store\\n  - explain agents and how to build one step-by-step\\n\\nWhich of those would you like next?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1096, 'prompt_tokens': 179, 'total_tokens': 1275, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Ctj71k9j9EVc9CgIg5JYhJqZRxInK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8119-7085-7190-b47f-78c93a55b3e4-0', usage_metadata={'input_tokens': 179, 'output_tokens': 1096, 'total_tokens': 1275, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is LangChain?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d9c32ccc-6691-4dd5-98dc-aabf02563ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful but sassy assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "921e808c-6618-4d2f-85df-1f3089497724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(langchain_core.messages.ai.AIMessage,\n",
       " [{'name': 'get_current_temperature',\n",
       "   'args': {'latitude': 37.7749, 'longitude': -122.4194},\n",
       "   'id': 'call_x2N78myfq2eHT9MPCCREvRI7',\n",
       "   'type': 'tool_call'}])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"What is the weather in SF right now?\"})\n",
    "type(result), result.tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "094baef1-4140-41f4-9111-ff9710826e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolMessage(content='The current temperature is 13.2°C', tool_call_id='call_x2N78myfq2eHT9MPCCREvRI7')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the tool call(s) produced by the model.\n",
    "tool_messages = execute_tool_calls(result, tools)\n",
    "tool_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f105e8e8-d418-4d4a-95eb-52636d4e890f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Right now in San Francisco it’s 13.2°C (about 55.8°F). Pretty brisk — sweater weather. Want more details (current conditions, humidity, wind, or a short-term forecast)?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 308, 'prompt_tokens': 78, 'total_tokens': 386, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Ctj7OHU1TJ7Gix9qQKlxrFeDpp7B2', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8119-cbe4-7bc3-92d1-b4bcec95f9a8-0', usage_metadata={'input_tokens': 78, 'output_tokens': 308, 'total_tokens': 386, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Optional) Send tool results back to the model to get a final natural-language answer.\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful but sassy assistant.\"),\n",
    "    HumanMessage(content=\"What is the weather in SF right now?\"),\n",
    "    result,\n",
    "    *tool_messages,\n",
    "]\n",
    "\n",
    "final_llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, disabled_params={\"stop\": None})\n",
    "final_llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "409b26c0-a1e0-4225-ae2e-396c1f76bf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Short answer: LangChain is an open-source framework for building applications that use large language models — especially apps that need to connect LLMs to data, tools, and multi-step logic (think RAG, chatbots with memory, agentic workflows, etc.).\\n\\nLonger answer (but still not a novel):\\n- Purpose: Make it easy to compose LLM calls, retrieval, tool use, state/memory, and orchestration into reliable applications rather than one-off prompts.\\n- Languages: Official SDKs for Python and JavaScript/TypeScript (most tooling and examples are in those).\\n\\nCore building blocks\\n- LLM wrappers: unified interfaces for model providers (OpenAI, Anthropic, Cohere, local models, etc.).\\n- Prompts & PromptTemplates: reusable prompt templates and prompt management.\\n- Chains: composable pipelines of steps (LLM → logic → retrieval → LLM, etc.).\\n- Agents & Tools: agents that plan which tools to call (web search, calculators, DB queries) and execute multi-step tasks.\\n- Memory: manage conversational state across turns (short-term, long-term).\\n- Document loaders / retrievers / vectorstores: load docs, embed them, store in vector DBs (Chroma, FAISS, Pinecone, Weaviate, etc.), and retrieve for RAG.\\n- Callbacks / tracing / logging: instrumentation for debugging and observability.\\n\\nTypical use cases\\n- Retrieval-Augmented Generation (RAG) — high-quality answers grounded in your data.\\n- Multi-step agentic workflows — fetch data, call APIs, compute, and report results.\\n- Conversational assistants with context/memory.\\n- Document understanding, summarization, Q&A over corpora.\\n- Automation pipelines that combine LLM reasoning with deterministic tools.\\n\\nHow it usually works (pattern)\\n1. Ingest documents → create embeddings → store in vector DB.\\n2. For a user query, retrieve relevant docs.\\n3. Feed retrieved context + prompt template to an LLM (chain).\\n4. Optionally let an agent call tools or follow multi-step plans.\\n5. Return answer and optionally save conversation to memory.\\n\\nPros\\n- Very modular and extensible.\\n- Huge ecosystem of integrations (models, vector DBs, tools).\\n- Makes complex flows and reproducibility easier.\\n- Large community and many examples.\\n\\nCaveats / limitations\\n- You still need to manage hallucinations, prompt quality, and costs.\\n- Operational complexity (latency, scaling, security) can rise with production use.\\n- Agents can behave unpredictably without careful constraints and tool design.\\n\\nTiny Python example (RAG-style)\\nfrom langchain import OpenAI, PromptTemplate\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.chains import RetrievalQA\\n\\nemb = OpenAIEmbeddings()\\nvectordb = Chroma.from_documents(docs, emb)\\nretriever = vectordb.as_retriever()\\nllm = OpenAI(temperature=0)\\nqa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\\nanswer = qa.run(\"What does the doc say about X?\")\\n\\nGetting started\\n- Official docs: docs.langchain.com (start with quickstart and examples).\\n- GitHub: search for LangChain repo for code and issues.\\n- Tutorials and community notebooks are abundant (Colab, Hugging Face spaces, etc.)\\n\\nIf you want, I can:\\n- sketch a small RAG demo you can run locally,\\n- show an agent example that calls web search and a calculator, or\\n- help pick the right vector DB and model provider for your scale and budget.\\n\\nWhich of those would make your life easier (or more dangerously automated)?'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple \"router\": if there are tool calls, execute them; otherwise return the text.\n",
    "def route_once(ai_message, tools):\n",
    "    if getattr(ai_message, \"tool_calls\", None):\n",
    "        tool_messages = execute_tool_calls(ai_message, tools)\n",
    "        return {\"ai_message\": ai_message, \"tool_messages\": tool_messages}\n",
    "    return {\"ai_message\": ai_message, \"tool_messages\": []}\n",
    "\n",
    "routed = route_once(chain.invoke({\"input\": \"What is LangChain?\"}), tools)\n",
    "routed[\"ai_message\"].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9cdde008-4b11-4ab2-a01c-6ae394a7dccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_current_temperature',\n",
       "  'args': {'latitude': 37.7749, 'longitude': -122.4194},\n",
       "  'id': 'call_twwbb1ZxEWTDH8GbTVSxXHKZ',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routed = route_once(chain.invoke({\"input\": \"What is the weather in San Francisco right now?\"}), tools)\n",
    "routed[\"ai_message\"].tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "66ffb308-3d77-4acb-b4b5-e2b0d38f3860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It’s 13.2°C (55.8°F) in San Francisco right now. Chilly by Bay standards—you might want a light jacket. Want an hourly forecast, rain chances, or wind info?'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the tool(s) and get a final answer in one helper.\n",
    "def answer_with_tools(user_input: str, tools):\n",
    "    llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, disabled_params={\"stop\": None}).bind_tools(tools)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful but sassy assistant.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ])\n",
    "    first = (prompt | llm).invoke({\"input\": user_input})\n",
    "    tool_messages = execute_tool_calls(first, tools)\n",
    "    if not tool_messages:\n",
    "        return first.content\n",
    "\n",
    "    final_llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, disabled_params={\"stop\": None})\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful but sassy assistant.\"),\n",
    "        HumanMessage(content=user_input),\n",
    "        first,\n",
    "        *tool_messages,\n",
    "    ]\n",
    "    return final_llm.invoke(messages).content\n",
    "\n",
    "answer_with_tools(\"What is the weather in San Francisco right now?\", tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "07e661b1-6d0d-43b9-9de4-19c6cceff291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Short answer: LangChain is an open-source framework for building applications that use large language models (LLMs). It doesn’t replace LLMs — it helps you orchestrate them, connect them to data and tools, and build reliable apps like chatbots, retrieval-augmented generation (RAG), agents, and automation workflows.\\n\\nWhat LangChain gives you\\n- Chains: Compose model calls, prompt templates, data transforms, and logic into reusable pipelines.\\n- Prompts & PromptTemplates: Manage and templatize prompts safely and consistently.\\n- Memory: Keep conversational state across interactions (short-term, long-term, or custom).\\n- Agents & Tools: Let models decide what actions to take (call APIs, run code, query DBs).\\n- Document loaders & text splitters: Ingest and prepare text from PDFs, web pages, etc.\\n- Embeddings & Vectorstores: Create embeddings and store/search them (Chroma, Pinecone, Milvus, Weaviate, Supabase, etc.).\\n- Retrievers & RAG: Combine retrieval with generation to ground answers on your data.\\n- Integrations: OpenAI, Azure, Anthropic, Hugging Face, and many DBs/indices/tooling.\\n\\nTypical flow (RAG example)\\n1. Load docs (PDFs, web pages).\\n2. Split into chunks and create embeddings.\\n3. Store embeddings in a vector database.\\n4. Given a user question, retrieve relevant chunks.\\n5. Feed retrieved context + prompt to an LLM to generate an answer.\\n\\nTiny Python-ish example (conceptual)\\nfrom langchain import OpenAI, PromptTemplate, LLMChain\\nprompt = PromptTemplate(\"Answer concisely: {question}\")\\nllm = OpenAI(temperature=0)\\nchain = LLMChain(llm=llm, prompt=prompt)\\nchain.run(question=\"What is LangChain?\")\\n\\nWhen to use LangChain\\n- You need structured workflows around LLMs (RAG, multi-step pipelines, tool use).\\n- You want built-in integrations for embeddings, vectorstores, and common data sources.\\n- You need conversation memory, agents, or production-ready orchestration.\\n\\nWhen it might be overkill\\n- Simple single-prompt use with no state or external data — a direct API call might be simpler.\\n- If you prefer very small, custom code and minimal dependencies.\\n\\nPitfalls & best practices\\n- Not an LLM: LangChain orchestrates; you still need to choose/tune an LLM.\\n- Watch costs and latency: multiple calls, large retrievals, and big context windows add both.\\n- Guard against hallucination: use retrieval grounding, verification steps, or tool-based checks.\\n- Security & privacy: don’t send sensitive data to third-party LLMs without controls.\\n- Keep prompts and chunk sizes tuned to your model’s context window.\\n\\nWhere to learn more\\n- Official docs: langchain.readthedocs.io (tutorials, integrations, examples)\\n- GitHub: the repo has examples and community-built connectors.\\n\\nThere — that’s LangChain in a nutshell. Want a tailored example (RAG, agent, or a specific vector DB)? I’ll build it for you — for free. Mostly.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_with_tools(\"What is LangChain?\", tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "057e3072-a203-4a5e-b1f6-b250cd7bd33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey! What’s up? I’m here and ready — what can I do for you today? (Ask me to summarize something, write an email, solve a problem, or just spill the tea.)'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_with_tools(\"hi!\", tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6bc0f-5b81-49fa-ac16-b8896847d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_with_tools(\"Search Wikipedia for LangChain and give me one sentence.\", tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd128d20-f552-4cc5-a45e-f47b58c9982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_with_tools(\"What's the temperature in London right now?\", tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f422a-b72b-40df-9552-cf9f9fc2d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also inspect the raw tool calls:\n",
    "llm_tools = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, disabled_params={\"stop\": None}).bind_tools(tools)\n",
    "raw = (prompt | llm_tools).invoke({\"input\": \"What's the weather in SF right now?\"})\n",
    "raw.tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefc329-b270-4ebb-b884-430e4e541e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_tool_calls(raw, tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dcb4b2-0e0d-425b-bff8-db7cd33afa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final answer after tool execution:\n",
    "user_input = \"What's the weather in SF right now?\"\n",
    "tool_msgs = execute_tool_calls(raw, tools)\n",
    "final_llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, disabled_params={\"stop\": None})\n",
    "final_llm.invoke([\n",
    "    SystemMessage(content=\"You are a helpful but sassy assistant.\"),\n",
    "    HumanMessage(content=user_input),\n",
    "    raw,\n",
    "    *tool_msgs,\n",
    "]).content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

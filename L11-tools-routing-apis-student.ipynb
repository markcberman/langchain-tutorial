{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b815ef73",
   "metadata": {},
   "source": [
    "# Tools and Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb41f5f4-df8d-4d04-9eaa-193b8c29b00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139c45e6-65a4-4fcb-a109-e5ba41a80835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a17f623-9342-46bd-a913-9ea6be32a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for weather online\"\"\"\n",
    "    return \"42f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db510949-acb8-4f69-b6c9-242b84fdc63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c12426-5e1a-4057-8ae5-6dfb6beace97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Search for weather online'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d1c4e6-8313-41a0-ae7a-61343699cb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'title': 'Query', 'type': 'string'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8765af16-17fc-4490-98ab-4e9dd59337cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"Thing to search for\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d20261-e18f-4989-808c-c4f36643d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(args_schema=SearchInput)\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for the weather online.\"\"\"\n",
    "    return \"42f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b3f7d0-a856-4842-82da-6c00df53e9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'description': 'Thing to search for',\n",
       "  'title': 'Query',\n",
       "  'type': 'string'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "060dc15b-f95f-47ab-b081-d6736f277ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42f'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.run(\"sf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e93b85b2-1786-4f63-8e65-cd6a8b5c0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    import requests\n",
    "    from datetime import datetime, UTC\n",
    "\n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"hourly\": \"temperature_2m\",\n",
    "        \"forecast_days\": 1,\n",
    "        \"timezone\": \"UTC\",  # helps make returned times consistent\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    response.raise_for_status()\n",
    "    results = response.json()\n",
    "\n",
    "    current_utc_time = datetime.now(UTC)\n",
    "\n",
    "    def _parse_to_utc(ts: str) -> datetime:\n",
    "        # Handle \"Z\" suffix if present\n",
    "        dt = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "        # If Open-Meteo returned a naive timestamp, assume it's UTC\n",
    "        if dt.tzinfo is None:\n",
    "            return dt.replace(tzinfo=UTC)\n",
    "        # Otherwise normalize to UTC\n",
    "        return dt.astimezone(UTC)\n",
    "\n",
    "    time_list = [_parse_to_utc(t) for t in results[\"hourly\"][\"time\"]]\n",
    "    temperature_list = results[\"hourly\"][\"temperature_2m\"]\n",
    "\n",
    "    closest_time_index = min(\n",
    "        range(len(time_list)),\n",
    "        key=lambda i: abs(time_list[i] - current_utc_time),\n",
    "    )\n",
    "\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    return f\"The current temperature is {current_temperature}°C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "731ed353-a27d-42df-89a8-9767bafb23be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4f62eb8-c528-4e4a-b078-22e9d9b20a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fetch current temperature for given coordinates.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38ebb59f-cf0f-4281-99c3-87c8cfd3377b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': {'description': 'Latitude of the location to fetch weather data for',\n",
       "  'title': 'Latitude',\n",
       "  'type': 'number'},\n",
       " 'longitude': {'description': 'Longitude of the location to fetch weather data for',\n",
       "  'title': 'Longitude',\n",
       "  'type': 'number'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36aacbec-9cdd-414a-b502-168cea350a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6277b6a8-f197-4057-a96a-5ebb61e18520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'get_current_temperature',\n",
       "  'description': 'Fetch current temperature for given coordinates.',\n",
       "  'parameters': {'properties': {'latitude': {'description': 'Latitude of the location to fetch weather data for',\n",
       "     'type': 'number'},\n",
       "    'longitude': {'description': 'Longitude of the location to fetch weather data for',\n",
       "     'type': 'number'}},\n",
       "   'required': ['latitude', 'longitude'],\n",
       "   'type': 'object'}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_openai_tool(get_current_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e088eceb-424b-4da6-981f-63322c9ac56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature is 33.9°C'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature.invoke({\"latitude\": 13, \"longitude\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5eefc272-d205-4bad-ab00-282ccb3cc2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    import wikipedia\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            self.wiki_client.exceptions.PageError,\n",
    "            self.wiki_client.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "289c5d0c-0e71-4d70-963f-9d9f58e53d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search_wikipedia'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69756d79-ec2f-4185-89af-d14de817fc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run Wikipedia search and get page summaries.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5b876cd-cc6c-484d-a3fa-d8810e22d564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'search_wikipedia',\n",
       "  'description': 'Run Wikipedia search and get page summaries.',\n",
       "  'parameters': {'properties': {'query': {'type': 'string'}},\n",
       "   'required': ['query'],\n",
       "   'type': 'object'}}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_openai_tool(search_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dfb10e9-5d1e-474a-9c8f-7abdf7939a34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: LangChain\\nSummary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\\n\\nPage: Vector database\\nSummary: A vector database, vector store or vector search engine is a database that stores and retrieves embeddings of data in vector space. Vector databases typically implement approximate nearest neighbor algorithms so users can search for records semantically similar to a given input, unlike traditional databases which primarily look up records by exact match. Use-cases for vector databases include similarity search, semantic search, multi-modal search, recommendations engines, object detection, and retrieval-augmented generation (RAG).\\nVector embeddings are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. Each data item is represented by one vector in this space. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\\nThese feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\\n\\nPage: Retrieval-augmented generation\\nSummary: Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information from external data sources. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM\\'s pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don\\'t exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.invoke({\"query\": \"langchain\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016ff34-bcdf-4ad3-98b6-da3917e29c92",
   "metadata": {},
   "source": [
    "### LangChain OpenAPI Tool Calling Using An OpenAPI Schema Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54cbe57e-f64c-4392-bcdd-9621fe1e46e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_community.agent_toolkits.json.toolkit import JsonToolkit\n",
    "from langchain_community.agent_toolkits.openapi.toolkit import RequestsToolkit\n",
    "from langchain_community.tools.json.tool import JsonSpec\n",
    "from langchain_community.utilities.requests import TextRequestsWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fe38e50-874c-49bd-8681-f95dcab0b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "{\n",
    "  \"openapi\": \"3.0.0\",\n",
    "  \"info\": {\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"title\": \"Swagger Petstore\",\n",
    "    \"license\": {\n",
    "      \"name\": \"MIT\"\n",
    "    }\n",
    "  },\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"url\": \"http://petstore.swagger.io/v1\"\n",
    "    }\n",
    "  ],\n",
    "  \"paths\": {\n",
    "    \"/pets\": {\n",
    "      \"get\": {\n",
    "        \"summary\": \"List all pets\",\n",
    "        \"operationId\": \"listPets\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"parameters\": [\n",
    "          {\n",
    "            \"name\": \"limit\",\n",
    "            \"in\": \"query\",\n",
    "            \"description\": \"How many items to return at one time (max 100)\",\n",
    "            \"required\": false,\n",
    "            \"schema\": {\n",
    "              \"type\": \"integer\",\n",
    "              \"maximum\": 100,\n",
    "              \"format\": \"int32\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"200\": {\n",
    "            \"description\": \"A paged array of pets\",\n",
    "            \"headers\": {\n",
    "              \"x-next\": {\n",
    "                \"description\": \"A link to the next page of responses\",\n",
    "                \"schema\": {\n",
    "                  \"type\": \"string\"\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Pets\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"post\": {\n",
    "        \"summary\": \"Create a pet\",\n",
    "        \"operationId\": \"createPets\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"201\": {\n",
    "            \"description\": \"Null response\"\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"/pets/{petId}\": {\n",
    "      \"get\": {\n",
    "        \"summary\": \"Info for a specific pet\",\n",
    "        \"operationId\": \"showPetById\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"parameters\": [\n",
    "          {\n",
    "            \"name\": \"petId\",\n",
    "            \"in\": \"path\",\n",
    "            \"required\": true,\n",
    "            \"description\": \"The id of the pet to retrieve\",\n",
    "            \"schema\": {\n",
    "              \"type\": \"string\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"200\": {\n",
    "            \"description\": \"Expected response to a valid request\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Pet\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"components\": {\n",
    "    \"schemas\": {\n",
    "      \"Pet\": {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\n",
    "          \"id\",\n",
    "          \"name\"\n",
    "        ],\n",
    "        \"properties\": {\n",
    "          \"id\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"format\": \"int64\"\n",
    "          },\n",
    "          \"name\": {\n",
    "            \"type\": \"string\"\n",
    "          },\n",
    "          \"tag\": {\n",
    "            \"type\": \"string\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"Pets\": {\n",
    "        \"type\": \"array\",\n",
    "        \"maxItems\": 100,\n",
    "        \"items\": {\n",
    "          \"$ref\": \"#/components/schemas/Pet\"\n",
    "        }\n",
    "      },\n",
    "      \"Error\": {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\n",
    "          \"code\",\n",
    "          \"message\"\n",
    "        ],\n",
    "        \"properties\": {\n",
    "          \"code\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"format\": \"int32\"\n",
    "          },\n",
    "          \"message\": {\n",
    "            \"type\": \"string\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "653156d4-d6a4-499b-aab3-cf4f7b1f0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the OpenAPI spec (as JSON here) and wrap it in a JsonSpec for the toolkit.\n",
    "spec_dict = json.loads(text)\n",
    "json_spec = JsonSpec(dict_=spec_dict, max_value_length=4000)\n",
    "\n",
    "# Requests wrapper (no auth needed for the Petstore example).\n",
    "requests_wrapper = TextRequestsWrapper(headers={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9ba6100-3c88-4b05-af93-784b57bf7424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " ['requests_get',\n",
       "  'requests_post',\n",
       "  'requests_patch',\n",
       "  'requests_put',\n",
       "  'requests_delete',\n",
       "  'json_spec_list_keys',\n",
       "  'json_spec_get_value']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a modern set of Tools for interacting with an OpenAPI-described API.\n",
    "#\n",
    "# NOTE: OpenAPIToolkit.get_tools() includes a `json_explorer` tool implemented as an *agent*.\n",
    "# That agent internally uses stop sequences, which GPT-5-mini rejects (no `stop` support).\n",
    "# To stay fully modern (tools-first) *and* compatible with GPT-5-mini, we compose:\n",
    "#   - RequestsToolkit: deterministic HTTP request tools (GET/POST/PATCH/PUT/DELETE)\n",
    "#   - JsonToolkit: deterministic tools for exploring the OpenAPI JSON spec\n",
    "# This avoids any legacy agent/function-calling shims and avoids passing `stop`.\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", disabled_params={\"stop\": None})\n",
    "\n",
    "requests_toolkit = RequestsToolkit(\n",
    "    requests_wrapper=requests_wrapper,\n",
    "    allow_dangerous_requests=True,\n",
    ")\n",
    "json_toolkit = JsonToolkit(spec=json_spec)\n",
    "\n",
    "openapi_tools = [*requests_toolkit.get_tools(), *json_toolkit.get_tools()]\n",
    "[len(openapi_tools), [t.name for t in openapi_tools]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91199697-c662-438f-8c68-e6daa8aad07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind the OpenAPI tools to the model (OpenAI \"tools\" schema).\n",
    "llm_with_openapi_tools = llm.bind_tools(openapi_tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ae8b3c1b-7a30-4b1b-abfd-e56f90b1d166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are three pet names: Bella, Max, Luna.\\n\\nWant names for a specific type (dog, cat, bird) or personality?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 706, 'total_tokens': 807, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Ctj6p9BHTmSokKljRABR4U3etmftS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8119-4009-7562-be6a-b43bf8f937fc-0', usage_metadata={'input_tokens': 706, 'output_tokens': 101, 'total_tokens': 807, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_openapi_tools.invoke(\"What are three pet names? (Call the API if you need to.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed100888-f3d4-4739-bba5-f839033850a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 706, 'total_tokens': 804, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CuJ1JXsZ4VLXn5OkbxJiv4lcTzC4I', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b8953-9bc4-7ed3-b6c1-1791b26ac6db-0', tool_calls=[{'name': 'requests_get', 'args': {'url': 'https://petstore.swagger.io/v2/pet/42'}, 'id': 'call_5eJwrEjqjmxfL4Uu6hcwfwFM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 706, 'output_tokens': 98, 'total_tokens': 804, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_openapi_tools.invoke(\"Tell me about the pet with id 42. (Use the API.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4063cdea-627c-4cc1-a2b5-9b5b6dcf179e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'requests_get',\n",
       "  'args': {'url': 'https://petstore.swagger.io/v2/pet/42'},\n",
       "  'id': 'call_xRAipkXG1pfNIsizBupcAl5f',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the model chose to call a tool, you'll see tool_calls on the AIMessage.\n",
    "msg = llm_with_openapi_tools.invoke(\"Look up the pet with id 42 and summarize it in one sentence.\")\n",
    "msg.tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a1f5534-cf06-4baf-b710-f169ea7434ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call name: requests_get and call args: {'url': 'https://petstore.swagger.io/v2/pet/42'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ToolMessage(content='{\"code\":1,\"type\":\"error\",\"message\":\"Pet not found\"}', tool_call_id='call_xRAipkXG1pfNIsizBupcAl5f')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper: execute tool calls locally (single-step execution).\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "def execute_tool_calls(ai_message: AIMessage, tools):\n",
    "    tool_map = {t.name: t for t in tools}\n",
    "    tool_messages = []\n",
    "    for call in ai_message.tool_calls or []:\n",
    "        name = call[\"name\"]\n",
    "        args = call.get(\"args\") or {}\n",
    "        print(f'call name: {call[\"name\"]} and call args: {call[\"args\"]}')\n",
    "        result = tool_map[name].invoke(args)\n",
    "        tool_messages.append(ToolMessage(content=str(result), tool_call_id=call[\"id\"]))\n",
    "    return tool_messages\n",
    "\n",
    "tool_messages = execute_tool_calls(msg, openapi_tools)\n",
    "tool_messages[:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b21dd-c9de-491d-9a0c-71ae56727689",
   "metadata": {},
   "source": [
    "### Routing (modern tool-calling)\n",
    "\n",
    "In lesson 3, we showed an example of **function calling** deciding between two candidate functions.\n",
    "\n",
    "In LangChain v1 (and OpenAI's current API), the modern pattern is **tool calling**:\n",
    "- You bind tools with `llm.bind_tools([...])`\n",
    "- The model returns `AIMessage.tool_calls`\n",
    "- Your application executes the tool(s) and optionally feeds results back to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8137758c-c5d3-47df-9062-5e31d43657e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'search_wikipedia',\n",
       "   'description': 'Run Wikipedia search and get page summaries.',\n",
       "   'parameters': {'properties': {'query': {'type': 'string'}},\n",
       "    'required': ['query'],\n",
       "    'type': 'object'}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'get_current_temperature',\n",
       "   'description': 'Fetch current temperature for given coordinates.',\n",
       "   'parameters': {'properties': {'latitude': {'description': 'Latitude of the location to fetch weather data for',\n",
       "      'type': 'number'},\n",
       "     'longitude': {'description': 'Longitude of the location to fetch weather data for',\n",
       "      'type': 'number'}},\n",
       "    'required': ['latitude', 'longitude'],\n",
       "    'type': 'object'}}}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "tools = [search_wikipedia, get_current_temperature]\n",
    "openai_tools_schema = [convert_to_openai_tool(t) for t in tools]\n",
    "openai_tools_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99120542-36bc-4ed1-aa9e-e2294e282c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, disabled_params={\"stop\": None}).bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a20047d9-4b34-4e6c-9407-570af1559bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 184, 'total_tokens': 283, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CuJMierIQXX1xpsnMxaKzFMTgLnHi', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b8967-dc92-7813-8e1b-fe8232022668-0', tool_calls=[{'name': 'get_current_temperature', 'args': {'latitude': 37.7749, 'longitude': -122.4194}, 'id': 'call_jDl0GpRawQzZBHw48KnSNWZy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 99, 'total_tokens': 283, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the weather in San Francisco right now?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "831a0506-1d5c-4bc3-b67f-24e201a4fa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Short answer\\n- LangChain is an open‑source framework for building applications that use large language models (LLMs). It provides reusable building blocks (chains, agents, memories, retrievers, vectorstores, etc.) so you can compose LLM calls with data, logic, and external tools.\\n\\nWhat it does (overview)\\n- Connects LLMs to data and to actions: lets models query documents (RAG), call APIs, run code, and maintain conversational state.\\n- Makes production LLM apps easier to build, test, and scale by providing abstractions for prompting, orchestration, and integrations.\\n\\nCore concepts\\n- LLM wrappers: unified interfaces to models (OpenAI, Hugging Face, local models).\\n- Prompts & prompt templates: parameterized prompt construction and management.\\n- Chains: sequences/compositions of calls (prompt → model → postprocess → next step).\\n- Agents & tools: let an LLM decide to call external tools/APIs or take multiple steps to solve a task.\\n- Memory: short- and long-term state for multi‑turn conversations.\\n- Retrievers & Vectorstores: search and retrieval (semantic search / RAG) using embeddings and vector databases (FAISS, Pinecone, Weaviate, Milvus, etc.).\\n- Document loaders: connectors to PDFs, web pages, databases, S3, etc.\\n- Callbacks & tracing: hooks for logging, metrics, debugging.\\n\\nCommon use cases\\n- Retrieval-Augmented Generation (RAG) — question answering over your documents.\\n- Conversational assistants with context and memory.\\n- Agentic automation — LLMs orchestrating API calls or multi-step workflows.\\n- Summarization, extraction, code generation, and data-aware apps.\\n\\nSimple examples\\nPython (pseudo):\\n- from langchain import OpenAI, PromptTemplate, LLMChain\\n- prompt = PromptTemplate(...)\\n- llm = OpenAI(...)\\n- chain = LLMChain(llm=llm, prompt=prompt)\\n- chain.run({\"input\": \"Summarize the text...\"})\\n\\nJavaScript/TypeScript (pseudo):\\n- import { OpenAI } from \"langchain\";\\n- const llm = new OpenAI();\\n- const response = await llm.call(\"Translate to French: Hello world\");\\n\\nEcosystem & integrations\\n- Supports many LLM providers and vector databases, and has connectors for document sources and cloud services.\\n- Works in Python and JavaScript/TypeScript (broader ecosystem and community tooling).\\n\\nAlternatives / complements\\n- LlamaIndex (focuses on indexing + RAG pipelines), Haystack, and other frameworks. They overlap; choice depends on priorities (indexing patterns, API style, features).\\n\\nWhen to use LangChain\\n- Building production LLM apps that need orchestration, data retrieval, tool usage, or conversational memory.\\n- When you want modular, reusable components for LLM workflows.\\n\\nCaveats\\n- LLM hallucinations, cost, latency, and data-privacy concerns still apply — LangChain helps integrate mitigations (RAG, tool use, checks) but doesn’t eliminate them.\\n- Complexity can grow for large agent workflows; design carefully.\\n\\nWhere to learn more\\n- Official docs and GitHub repo (search “LangChain docs” or “LangChain GitHub”) have guides, examples, and tutorials.\\n\\nIf you want, I can:\\n- Show a runnable minimal example in Python or JavaScript.\\n- Explain how to build a RAG pipeline step by step.\\n- Compare LangChain vs LlamaIndex for a specific use case. Which would you like?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 992, 'prompt_tokens': 179, 'total_tokens': 1171, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CuJNAiZdyx4BJ56XkTiNZyzVB5FSE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8968-48e4-7601-8ab2-8875d0f53241-0', usage_metadata={'input_tokens': 179, 'output_tokens': 992, 'total_tokens': 1171, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is LangChain?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9c32ccc-6691-4dd5-98dc-aabf02563ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful but sassy assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "921e808c-6618-4d2f-85df-1f3089497724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(langchain_core.messages.ai.AIMessage,\n",
       " [{'name': 'get_current_temperature',\n",
       "   'args': {'latitude': 37.7749, 'longitude': -122.4194},\n",
       "   'id': 'call_iaVmg0oM9WvfHm6MhXN2EbxE',\n",
       "   'type': 'tool_call'}])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"What is the weather in SF right now?\"})\n",
    "type(result), result.tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "094baef1-4140-41f4-9111-ff9710826e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call name: get_current_temperature and call args: {'latitude': 37.7749, 'longitude': -122.4194}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ToolMessage(content='The current temperature is 12.1°C', tool_call_id='call_iaVmg0oM9WvfHm6MhXN2EbxE')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the tool call(s) produced by the model.\n",
    "tool_messages = execute_tool_calls(result, tools)\n",
    "tool_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f105e8e8-d418-4d4a-95eb-52636d4e890f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Right now in San Francisco it’s 12.1°C (53.8°F). Pretty brisk — a light jacket is recommended. \\n\\nWant current conditions (clouds, rain, wind) or a forecast for the day? I can pull that too.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Optional) Send tool results back to the model to get a final natural-language answer.\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful but sassy assistant.\"),\n",
    "    HumanMessage(content=\"What is the weather in SF right now?\"),\n",
    "    result,\n",
    "    *tool_messages,\n",
    "]\n",
    "\n",
    "final_llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, disabled_params={\"stop\": None})\n",
    "final_llm.invoke(messages).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "409b26c0-a1e0-4225-ae2e-396c1f76bf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Short answer (because you asked nicely): LangChain is an open-source framework / SDK for building applications that use large language models (LLMs). Think of it as the glue, batteries, and instruction manual for turning an LLM into a useful app — chatbots, retrieval-augmented generation (RAG), multi-step agents, pipelines, and more.\\n\\nQuick breakdown — what LangChain gives you\\n- Core idea: compose LLM calls with other pieces (prompts, data, tools, memory, logic) into reusable \"chains\" and \"agents\".\\n- Key components:\\n  - LLM wrappers: unify access to different models (OpenAI, Anthropic, local LLMs, etc.).\\n  - Prompts & prompt templates: manage and parametrize prompts cleanly.\\n  - Chains: compose multiple steps (e.g., call LLM → process → call LLM again).\\n  - Agents: let the model decide which tools to call (search, calculator, browser, APIs) and orchestrate multi-step workflows.\\n  - Memory: keep conversation or application state between calls.\\n  - Connectors / Loaders: import documents (PDFs, web pages, databases) and build indexes.\\n  - Vectorstores & retrievers: store embeddings and retrieve relevant docs for RAG.\\n  - Callbacks & instrumentation: logging, tracing, and debugging of flows.\\n- Languages/SDKs: official SDKs for Python and JavaScript/TypeScript (largest ecosystem in Python).\\n\\nCommon use cases\\n- Retrieval-augmented generation (RAG): answer questions from your documents.\\n- Conversational agents: chatbots that can use tools to perform tasks.\\n- Workflows that chain multiple LLM calls with business logic, APIs, or external data.\\n- Summarization, semantic search, Q&A over corpora.\\n\\nWhy people use it\\n- Makes complex LLM-based applications less ad hoc and more maintainable.\\n- Provides reusable building blocks and integrations, saving you from wiring everything yourself.\\n- Large community, lots of examples, and integrations (vector DBs, cloud providers, model vendors).\\n\\nTiny Python example (conceptual)\\n- Create a prompt template, wrap an LLM, make a simple chain:\\n  - template = \"Summarize: {text}\"\\n  - llm = OpenAI(...)\\n  - chain = LLMChain(prompt=template, llm=llm)\\n  - chain.run(text=\"Long document text...\")\\n\\nWhere to look next\\n- Official docs: docs.langchain.com\\n- GitHub and LangChain Hub for examples, components, and community-built integrations.\\n\\nWant a specific example (RAG, agent, or a small app) or a code snippet in Python or JavaScript? I can whip one up — faster than you can say \"semantic search.\"'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple \"router\": if there are tool calls, execute them; otherwise return the text.\n",
    "def route_once(ai_message, tools):\n",
    "    if getattr(ai_message, \"tool_calls\", None):\n",
    "        tool_messages = execute_tool_calls(ai_message, tools)\n",
    "        return {\"ai_message\": ai_message, \"tool_messages\": tool_messages}\n",
    "    return {\"ai_message\": ai_message, \"tool_messages\": []}\n",
    "\n",
    "routed = route_once(chain.invoke({\"input\": \"What is LangChain?\"}), tools)\n",
    "routed[\"ai_message\"].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9cdde008-4b11-4ab2-a01c-6ae394a7dccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call name: get_current_temperature and call args: {'latitude': 37.7749, 'longitude': -122.4194}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_current_temperature',\n",
       "  'args': {'latitude': 37.7749, 'longitude': -122.4194},\n",
       "  'id': 'call_TJgyFFYZYSKA7x35ZHKJGkvt',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routed = route_once(chain.invoke({\"input\": \"What is the weather in San Francisco right now?\"}), tools)\n",
    "routed[\"ai_message\"].tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66ffb308-3d77-4acb-b4b5-e2b0d38f3860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call name: get_current_temperature and call args: {'latitude': 37.7749, 'longitude': -122.4194}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It’s 12.1°C (53.8°F) in San Francisco right now — pleasantly cool. \\n\\nI only have the current temperature from my check. Want current conditions (cloudy/rain/wind), humidity, an hourly forecast, or radar?'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the tool(s) and get a final answer in one helper.\n",
    "def answer_with_tools(user_input: str, tools):\n",
    "    llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, disabled_params={\"stop\": None}).bind_tools(tools)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful but sassy assistant.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ])\n",
    "    first = (prompt | llm).invoke({\"input\": user_input})\n",
    "    tool_messages = execute_tool_calls(first, tools)\n",
    "    if not tool_messages:\n",
    "        return first.content\n",
    "\n",
    "    final_llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, disabled_params={\"stop\": None})\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful but sassy assistant.\"),\n",
    "        HumanMessage(content=user_input),\n",
    "        first,\n",
    "        *tool_messages,\n",
    "    ]\n",
    "    return final_llm.invoke(messages).content\n",
    "\n",
    "answer_with_tools(\"What is the weather in San Francisco right now?\", tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07e661b1-6d0d-43b9-9de4-19c6cceff291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Short answer: LangChain is an open‑source framework for building applications that use large language models (LLMs). Think of it as a batteries‑included toolkit that connects LLMs to data, tools, memory, and orchestration so you can build chatbots, retrieval‑augmented QA, agents that call APIs, and other LLM-powered apps faster.\\n\\nWhat it gives you (high level)\\n- Connectors: easy integrations with LLM providers (OpenAI, Anthropic, Hugging Face, etc.), vector databases (Pinecone, Chroma, FAISS, Weaviate, Milvus), search, file stores and other tools.\\n- Chains: composable building blocks that link prompt templates, LLM calls, retrievers, and post‑processing into workflows.\\n- Prompts & templates: structured prompt handling, few‑shot examples, and prompt optimization helpers.\\n- Retrieval & RAG: retrievers + vectorstores for retrieval‑augmented generation (search docs and feed results into the LLM).\\n- Agents & tools: let an LLM plan and call external tools/APIs (search, calculators, custom functions) in multiple steps.\\n- Memory: short/long/summary conversational memory to keep context across interactions.\\n- Utilities: callbacks, tracing, evaluation helpers, and standard components for common tasks.\\n\\nTypical architecture (example)\\nUser -> UI -> Chain:\\n- Retriever queries vector store for relevant docs\\n- LLM + prompt template produces answer (possibly using tool calls)\\n- Memory stores conversation state\\n-> Response\\n\\nTiny Python example (conceptual)\\n- pip install langchain, openai, chromadb\\n- create a retriever from a vectorstore, pass it + a prompt to a QA chain, run it.\\n\\nWhy use it\\n- Speeds development of production LLM apps by providing reusable patterns and integrations.\\n- Encourages best practices (RAG, structured prompts, tool safety).\\n- Active community, many examples and templates.\\n\\nWhere to start\\n- Docs & examples: https://docs.langchain.com\\n- GitHub: search “langchain” for repo and examples\\n- Install: pip install langchain (or npm package for JS/TS)\\n\\nAnything specific you want—Python vs JS examples, building a RAG QA bot, or how agents work? I can show code or a simple architecture for your use case.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_with_tools(\"What is LangChain?\", tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "057e3072-a203-4a5e-b1f6-b250cd7bd33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well hello! What's up — what can I help you with today?\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_with_tools(\"hi!\", tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5be6bc0f-5b81-49fa-ac16-b8896847d87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call name: search_wikipedia and call args: {'query': 'LangChain'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to Wikipedia, LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_with_tools(\"Search Wikipedia for LangChain and give me one sentence.\", tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd128d20-f552-4cc5-a45e-f47b58c9982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call name: get_current_temperature and call args: {'latitude': 51.5074, 'longitude': -0.1278}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It’s 2.0°C in London right now — that’s about 35.6°F. Brr… you’ll want a coat. Want a forecast or rain check, too?'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_with_tools(\"What's the temperature in London right now?\", tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "134f422a-b72b-40df-9552-cf9f9fc2d780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_current_temperature',\n",
       "  'args': {'latitude': 37.7749, 'longitude': -122.4194},\n",
       "  'id': 'call_6sSSbGrle7IGisIrLarpEXoz',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also inspect the raw tool calls:\n",
    "llm_tools = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, disabled_params={\"stop\": None}).bind_tools(tools)\n",
    "raw = (prompt | llm_tools).invoke({\"input\": \"What's the weather in SF right now?\"})\n",
    "raw.tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8fefc329-b270-4ebb-b884-430e4e541e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call name: get_current_temperature and call args: {'latitude': 37.7749, 'longitude': -122.4194}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ToolMessage(content='The current temperature is 12.1°C', tool_call_id='call_6sSSbGrle7IGisIrLarpEXoz')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_tool_calls(raw, tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2dcb4b2-0e0d-425b-bff8-db7cd33afa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call name: get_current_temperature and call args: {'latitude': 37.7749, 'longitude': -122.4194}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's 12.1°C (53.8°F) in San Francisco right now. I only have the temperature at the moment — want me to pull current conditions (clouds, wind, rain) or the forecast so you can decide whether to grab a jacket?\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final answer after tool execution:\n",
    "user_input = \"What's the weather in SF right now?\"\n",
    "tool_msgs = execute_tool_calls(raw, tools)\n",
    "final_llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, disabled_params={\"stop\": None})\n",
    "final_llm.invoke([\n",
    "    SystemMessage(content=\"You are a helpful but sassy assistant.\"),\n",
    "    HumanMessage(content=user_input),\n",
    "    raw,\n",
    "    *tool_msgs,\n",
    "]).content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
